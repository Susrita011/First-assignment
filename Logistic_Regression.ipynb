{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoWhDSEZkWd8"
      },
      "source": [
        "THEORITICAL QUESTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3ofon5wkd53"
      },
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FZPJPgIkwjL"
      },
      "source": [
        "ans . Logistic Regression is a statistical method used for binary classification problems, where the goal is to predict the probability of a binary outcome (e.g., yes/no, 0/1) based on a set of input features. It's a popular and interpretable algorithm, often used in various fields like healthcare, finance, and marketing.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Sigmoid Function:** Logistic Regression uses the sigmoid function to map the linear combination of input features to a probability between 0 and 1. The sigmoid function is an S-shaped curve that ensures the output is always within this range.\n",
        "2. **Linear Combination:** It calculates a weighted sum of the input features, similar to Linear Regression. However, instead of directly outputting this sum, it passes it through the sigmoid function.\n",
        "3. **Probability Threshold:** Based on the predicted probability, a threshold is set to classify the outcome. If the probability is above the threshold (usually 0.5), the outcome is classified as one category (e.g., 1 or \"yes\"), and if it's below, it's classified as the other category (e.g., 0 or \"no\").\n",
        "4. **Model Training:** The model is trained using labeled data, where the algorithm learns the optimal weights for the input features that minimize the difference between the predicted probabilities and the actual outcomes.\n",
        "\n",
        "**Differences from Linear Regression:**\n",
        "\n",
        "| Feature | Logistic Regression | Linear Regression |\n",
        "|---|---|---|\n",
        "| **Output** | Probability of a binary outcome (0-1) | Continuous value |\n",
        "| **Type of Problem** | Binary classification | Regression |\n",
        "| **Function** | Sigmoid function | Linear function |\n",
        "| **Use Case** | Predicting categories (e.g., spam/not spam) | Predicting values (e.g., house prices) |\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "- Linear Regression predicts a continuous value, while Logistic Regression predicts the probability of a binary outcome.\n",
        "- Linear Regression uses a linear function to model the relationship between features and the outcome, while Logistic Regression uses the sigmoid function.\n",
        "- Linear Regression is used for regression tasks, while Logistic Regression is used for classification tasks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU_dfjJMlZBg"
      },
      "source": [
        "2. What is the mathematical equation of Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4YC8qWAmLkY"
      },
      "source": [
        "ans. The mathematical equation for Logistic Regression can be expressed in a few ways, but here's the most common and fundamental one:\n",
        "\n",
        "**Probability of a positive outcome (y=1):**\n",
        "\n",
        "```\n",
        "P(y=1 | x) = 1 / (1 + exp(-z))\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "*  `P(y=1 | x)` is the probability of the outcome being 1 (e.g., \"yes\", \"success\") given the input features `x`.\n",
        "*  `x` is a vector of input features (independent variables).\n",
        "*  `exp()` is the exponential function (e raised to the power of).\n",
        "*  `z` is the linear combination of the input features and their weights:\n",
        "\n",
        "```\n",
        "z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "*  `w₀` is the intercept (or bias).\n",
        "*  `w₁, w₂, ..., wₙ` are the weights (or coefficients) associated with the input features.\n",
        "*  `x₁, x₂, ..., xₙ` are the values of the input features.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "1. **Linear Combination:** The equation first calculates a weighted sum of the input features (`z`), similar to linear regression. Each feature is multiplied by its corresponding weight, and these products are added together along with the intercept.\n",
        "\n",
        "2. **Sigmoid Function:** This linear combination (`z`) is then passed through the sigmoid function (the `1 / (1 + exp(-z))` part). The sigmoid function squashes any real-valued number into a probability between 0 and 1.\n",
        "\n",
        "**Why this equation works:**\n",
        "\n",
        "* **Probability:** The sigmoid function ensures that the output is always a valid probability (between 0 and 1).\n",
        "* **Linearity in the Log-Odds:** If you take the log of the odds (the logit) of the probability, you get a linear combination of the input features. This means that Logistic Regression models a linear relationship between the features and the log-odds of the outcome.\n",
        "* **Interpretability:** The weights (`wᵢ`) can be interpreted as the change in the log-odds of the outcome for a one-unit change in the corresponding feature, holding other features constant.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "The Logistic Regression equation combines a linear combination of input features with the sigmoid function to produce a probability of a binary outcome.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV6jpAmJmZJf"
      },
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELku7iCBmoPF"
      },
      "source": [
        "ans. The sigmoid function is a crucial component of Logistic Regression, and here's why:\n",
        "\n",
        "**1. Probability Output:**\n",
        "\n",
        "- Logistic Regression aims to predict the probability of a binary outcome (e.g., yes/no, 0/1). The sigmoid function's unique property is that it maps any real-valued number to a value between 0 and 1. This perfectly aligns with the concept of probability, which always falls within this range.\n",
        "\n",
        "**2. Non-linearity:**\n",
        "\n",
        "- The sigmoid function introduces non-linearity into the model. This is essential because many real-world relationships between features and outcomes are not linear. The non-linearity allows Logistic Regression to model more complex patterns in the data.\n",
        "\n",
        "**3. Smoothness and Differentiability:**\n",
        "\n",
        "- The sigmoid function is a smooth and differentiable function. This is important for the optimization algorithms used to train the Logistic Regression model. These algorithms rely on gradients (derivatives) to find the optimal weights for the features, and the smoothness of the sigmoid function ensures that these gradients are well-behaved.\n",
        "\n",
        "**4. Interpretability:**\n",
        "\n",
        "- The output of the sigmoid function can be directly interpreted as the probability of the positive class (y=1). This makes it easy to understand the model's predictions and to make decisions based on the predicted probabilities.\n",
        "\n",
        "**5. Historical Reasons:**\n",
        "\n",
        "- The sigmoid function has been used in various fields for a long time, including statistics and biology. It was a natural choice to use it in Logistic Regression when it was first developed.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "The sigmoid function is used in Logistic Regression because it:\n",
        "\n",
        "- Produces probability outputs between 0 and 1.\n",
        "- Introduces non-linearity to model complex relationships.\n",
        "- Is smooth and differentiable for optimization.\n",
        "- Provides interpretable probabilities.\n",
        "\n",
        "While other functions could potentially be used, the sigmoid function has proven to be a very effective and widely used choice for Logistic Regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ati5SpSpm4eQ"
      },
      "source": [
        "4. What is the cost function of Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhGkXt6cm_TT"
      },
      "source": [
        "ans. The cost function for Logistic Regression is called **Log Loss** or **Cross-Entropy Loss**. It measures the difference between the predicted probabilities and the actual outcomes.\n",
        "\n",
        "It uses logarithms to penalize incorrect predictions more heavily, ensuring the model learns to make accurate probability estimations.\n",
        "\n",
        "**Why this specific cost function?**\n",
        "\n",
        "* **Convexity:** It's a convex function, meaning it has a single minimum point. This is crucial for optimization algorithms to find the best model parameters.\n",
        "* **Sensitivity to Errors:** It heavily penalizes confident but wrong predictions, encouraging the model to be more cautious.\n",
        "\n",
        "**The goal:**\n",
        "\n",
        "The goal during training is to minimize this cost function, leading to a model that makes accurate predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZGoTU6YngW7"
      },
      "source": [
        "5. What is Regularization in Logistic Regression/ Why is it needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy24vym2numh"
      },
      "source": [
        "In Logistic Regression, regularization is a technique used to prevent **overfitting**.\n",
        "\n",
        "**What is Overfitting?**\n",
        "\n",
        "Overfitting happens when your model learns the training data *too* well, including its noise and outliers. This leads to a model that performs great on the training data but poorly on new, unseen data. It's like memorizing answers for a test instead of understanding the concepts.\n",
        "\n",
        "**How Regularization Helps**\n",
        "\n",
        "Regularization adds a penalty to the cost function of Logistic Regression. This penalty discourages the model from having very large weights for the features. By keeping the weights smaller, the model becomes simpler and less prone to overfitting.\n",
        "\n",
        "**Why is it Needed?**\n",
        "\n",
        "* **Improved Generalization:** Regularization helps the model generalize better to new data, making it more reliable in real-world applications.\n",
        "* **Handles High Dimensionality:** When you have many features (high dimensionality), regularization becomes especially important to prevent the model from becoming too complex and overfitting.\n",
        "\n",
        "**Types of Regularization**\n",
        "\n",
        "There are two main types of regularization used in Logistic Regression:\n",
        "\n",
        "* **L1 Regularization (Lasso):** This type can actually shrink some feature weights to zero, effectively performing feature selection.\n",
        "* **L2 Regularization (Ridge):** This type shrinks the weights towards zero but usually doesn't eliminate them completely.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "Regularization is like adding a \"complexity control\" to your Logistic Regression model. It helps the model find a sweet spot where it learns the underlying patterns in the data without memorizing the noise, leading to better predictions on unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiDmaAsToQ6T"
      },
      "source": [
        "6. Explain the difference between Lasso, Ridge, and elastic Net regression ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY1KmVGbor_Y"
      },
      "source": [
        "Lasso, Ridge, and Elastic Net are all regularization techniques used in linear regression to prevent overfitting and improve the model's generalization ability. They achieve this by adding a penalty term to the cost function, which discourages the model from having very large weights for the features. Here's a breakdown of their differences:\n",
        "\n",
        "**1. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "* **Penalty:** Adds the sum of the *squares* of the coefficients to the cost function.\n",
        "* **Effect:** Shrinks the coefficients towards zero, but usually doesn't eliminate them completely.\n",
        "* **Use Case:** Useful when you have many features, some of which might be correlated. Ridge helps to reduce the impact of less important features without completely removing them.\n",
        "\n",
        "**2. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "* **Penalty:** Adds the sum of the *absolute values* of the coefficients to the cost function.\n",
        "* **Effect:** Can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "* **Use Case:** Useful when you suspect that only a few features are truly important, and the rest are noise or redundant. Lasso helps to identify and select the most relevant features.\n",
        "\n",
        "**3. Elastic Net Regression**\n",
        "\n",
        "* **Penalty:** Combines both L1 (Lasso) and L2 (Ridge) penalties in the cost function.\n",
        "* **Effect:** Balances the properties of Lasso and Ridge. It can perform feature selection like Lasso while also handling multicollinearity like Ridge.\n",
        "* **Use Case:** Useful when you have many features, some of which might be correlated, and you also want to perform feature selection. Elastic Net provides a more stable and robust model compared to using Lasso or Ridge alone in such situations.\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature | Ridge Regression | Lasso Regression | Elastic Net Regression |\n",
        "|---|---|---|---|\n",
        "| Penalty | L2 (sum of squared coefficients) | L1 (sum of absolute values of coefficients) | L1 + L2 |\n",
        "| Coefficient Shrinkage | Shrinks coefficients towards zero | Shrinks coefficients towards zero, can shrink some to zero | Shrinks coefficients towards zero, can shrink some to zero |\n",
        "| Feature Selection | No | Yes | Yes |\n",
        "| Handles Multicollinearity | Yes | To some extent | Yes |\n",
        "| Use Case | Many features, some correlated | Few important features, others noise | Many features, some correlated, feature selection needed |\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "* Ridge is like a gentle nudge, reducing the impact of less important features.\n",
        "* Lasso is like a stricter teacher, kicking out the least useful features.\n",
        "* Elastic Net is like a wise mediator, balancing the strengths of both Ridge and Lasso.\n",
        "\n",
        "The choice between these techniques depends on the specific dataset and the goals of the analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Vi6m5No42N"
      },
      "source": [
        "7. When should we use elastic Net instead of Lasso or Ridge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULXUCrH9pH0A"
      },
      "source": [
        "Elastic Net is a powerful regularization technique that combines the strengths of both Lasso and Ridge regression. It's particularly useful in situations where you face challenges that neither Lasso nor Ridge can fully address on their own. Here's when you should consider using Elastic Net:\n",
        "\n",
        "**1. When you have many features, some of which are correlated:**\n",
        "\n",
        "- **Lasso's Limitation:** Lasso excels at feature selection, but it can struggle with correlated features. It might arbitrarily pick one feature from a group of correlated features and discard the others, even if they all carry valuable information.\n",
        "- **Ridge's Limitation:** Ridge handles multicollinearity well, but it doesn't perform feature selection. It keeps all features, even if they are not very important.\n",
        "- **Elastic Net's Advantage:** Elastic Net combines the best of both worlds. It can handle correlated features like Ridge while still performing feature selection like Lasso. It's more likely to keep groups of correlated features together, rather than arbitrarily discarding some of them.\n",
        "\n",
        "**2. When you want a balance between feature selection and coefficient shrinkage:**\n",
        "\n",
        "- **Lasso's Focus:** Lasso is heavily focused on feature selection. It can aggressively shrink coefficients to zero, potentially removing useful features if not tuned carefully.\n",
        "- **Ridge's Focus:** Ridge primarily focuses on shrinking coefficients. It keeps all features, which might not be ideal when you have many irrelevant features.\n",
        "- **Elastic Net's Balance:** Elastic Net allows you to control the balance between feature selection and coefficient shrinkage. You can tune the parameters to favor either feature selection or coefficient shrinkage, depending on your needs.\n",
        "\n",
        "**3. When you're not sure whether feature selection is crucial:**\n",
        "\n",
        "- **Lasso's Assumption:** Lasso assumes that only a few features are truly important. If this assumption doesn't hold, Lasso might remove valuable features.\n",
        "- **Ridge's Approach:** Ridge keeps all features, which might lead to a more complex model with reduced interpretability.\n",
        "- **Elastic Net's Flexibility:** Elastic Net provides a more flexible approach. You can start with Elastic Net and then tune the parameters to see if feature selection improves the model's performance.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Elastic Net is a versatile regularization technique that's particularly useful when you have a dataset with many features, some of which are correlated, and you want to balance feature selection with coefficient shrinkage. It offers a more robust and flexible approach compared to using Lasso or Ridge alone in such situations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um2bXQjpEdAD"
      },
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xE-1M5_EqpZ"
      },
      "source": [
        "ans. The regularization parameter, often denoted as λ (lambda), plays a crucial role in Logistic Regression by controlling the strength of regularization. It directly influences the trade-off between model complexity and its ability to fit the training data. Here's a breakdown of its impact:\n",
        "\n",
        "**1. Controlling Model Complexity:**\n",
        "\n",
        "- **Higher λ:** Increasing the value of λ strengthens the regularization effect. This leads to a simpler model with smaller coefficients. The model becomes less prone to overfitting, as it's discouraged from learning intricate details of the training data, including noise and outliers.\n",
        "- **Lower λ:** Decreasing λ weakens regularization. The model becomes more complex, with larger coefficients. It can fit the training data more closely, potentially capturing even the noise, which increases the risk of overfitting.\n",
        "\n",
        "**2. Balancing Bias and Variance:**\n",
        "\n",
        "- **Higher λ:** Stronger regularization (high λ) increases the bias of the model. It becomes more constrained and might underfit the data if λ is too high. Underfitting occurs when the model is too simple to capture the underlying patterns in the data.\n",
        "- **Lower λ:** Weaker regularization (low λ) reduces bias but increases variance. The model becomes more flexible and can fit the training data very well. However, this can lead to high variance, meaning the model is very sensitive to the specific training data and might not generalize well to unseen data.\n",
        "\n",
        "**3. Impact on Coefficients:**\n",
        "\n",
        "- **Higher λ:** Larger λ values push the coefficients towards zero. In L1 regularization (Lasso), some coefficients might even become exactly zero, effectively performing feature selection. In L2 regularization (Ridge), coefficients become smaller but rarely reach zero.\n",
        "- **Lower λ:** Smaller λ values allow the coefficients to grow larger. The model relies more on the features and can potentially overfit if λ is too low.\n",
        "\n",
        "**4. Finding the Optimal λ:**\n",
        "\n",
        "- The optimal value of λ depends on the specific dataset and the goals of the analysis. It's typically determined using techniques like cross-validation. Cross-validation involves splitting the data into multiple subsets, training the model with different λ values on some subsets, and evaluating its performance on the remaining subsets. The λ value that results in the best performance on the validation sets is chosen as the optimal value.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "The regularization parameter λ controls the balance between model complexity and its ability to fit the training data. A higher λ leads to a simpler model with smaller coefficients, reducing the risk of overfitting but potentially increasing bias. A lower λ makes the model more complex, increasing the risk of overfitting but reducing bias. Finding the optimal λ is crucial for building a well-performing Logistic Regression model that generalizes well to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvRdkTSPE4zr"
      },
      "source": [
        "9. What are the key assumptions of Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEC_yMqGFCOh"
      },
      "source": [
        "Logistic regression, while a powerful and versatile tool, relies on certain key assumptions to ensure its results are valid and reliable. Here are the main ones:\n",
        "\n",
        "**1. Binary Outcome:**\n",
        "\n",
        "- The dependent variable (the one you're trying to predict) must be binary or dichotomous. This means it can only take two values, such as 0 or 1, yes or no, true or false.\n",
        "\n",
        "**2. Independence of Observations:**\n",
        "\n",
        "- The observations in your dataset should be independent of each other. This means that each data point should be unrelated to the others. For example, if you're analyzing customer behavior, each customer's actions should not influence another customer's actions.\n",
        "\n",
        "**3. No Multicollinearity:**\n",
        "\n",
        "- The independent variables (the ones you're using to make predictions) should not be highly correlated with each other. Multicollinearity can distort the results and make it difficult to determine the individual effect of each variable on the outcome.\n",
        "\n",
        "**4. Linearity of the Logit:**\n",
        "\n",
        "- While logistic regression doesn't assume a linear relationship between the independent variables and the outcome itself, it does assume a linear relationship between the independent variables and the *log-odds* (or logit) of the outcome. This means that if you were to plot the log-odds of the outcome against each independent variable, you should see a roughly linear pattern.\n",
        "\n",
        "**5. Sufficient Sample Size:**\n",
        "\n",
        "- Logistic regression generally requires a larger sample size compared to linear regression. A common rule of thumb is to have at least 10-20 events (the less frequent outcome) per independent variable in your model.\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "- **These are assumptions, not strict requirements:** While it's ideal for these assumptions to hold, logistic regression can still be useful even if some assumptions are slightly violated. However, significant violations can lead to misleading results.\n",
        "- **Checking assumptions is crucial:** Before drawing conclusions from your logistic regression model, it's essential to check whether these assumptions are met. There are various diagnostic tools and techniques available to assess these assumptions.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "By understanding and checking these key assumptions, you can ensure that your logistic regression model is appropriate for your data and that the results you obtain are reliable and meaningful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M1VsRb8FHrK"
      },
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGapvf2IFUei"
      },
      "source": [
        "ans. You're right to think beyond logistic regression! While it's a workhorse, there are many other classification algorithms with their own strengths. Here are some popular alternatives:\n",
        "\n",
        "**1. Tree-Based Methods:**\n",
        "\n",
        "* **Decision Trees:** These create a set of rules to classify data based on feature values. They are easy to visualize and interpret but can be prone to overfitting.\n",
        "* **Random Forests:** This is an ensemble method that combines multiple decision trees. It's more robust than a single tree and often provides higher accuracy.\n",
        "* **Gradient Boosting Machines (GBM):** Another ensemble method that builds trees sequentially, with each tree correcting the errors of the previous ones. Popular GBM implementations include XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "**2. Support Vector Machines (SVM):**\n",
        "\n",
        "* SVMs find an optimal hyperplane that maximally separates data points of different classes. They are effective in high-dimensional spaces and can handle non-linear relationships using kernel tricks.\n",
        "\n",
        "**3. K-Nearest Neighbors (KNN):**\n",
        "\n",
        "* KNN classifies a data point based on the classes of its k-nearest neighbors in the feature space. It's simple to understand and implement but can be computationally expensive for large datasets.\n",
        "\n",
        "**4. Naive Bayes:**\n",
        "\n",
        "* Based on Bayes' theorem, Naive Bayes assumes that features are independent of each other. It's computationally efficient and works well for high-dimensional data, such as text classification.\n",
        "\n",
        "**5. Neural Networks:**\n",
        "\n",
        "* Neural networks are complex models inspired by the human brain. They can learn very complex patterns and are particularly useful for tasks like image recognition and natural language processing.\n",
        "\n",
        "**6. Other Notable Alternatives:**\n",
        "\n",
        "* **Discriminant Analysis:** A statistical method that finds linear combinations of features to separate classes.\n",
        "* **Rule-Based Classifiers:** These use a set of predefined rules to classify data.\n",
        "* **Bayesian Networks:** These represent probabilistic relationships between variables and can be used for classification.\n",
        "\n",
        "**Choosing the Right Algorithm:**\n",
        "\n",
        "The best alternative to logistic regression depends on several factors, including:\n",
        "\n",
        "* **Data characteristics:** The size, type, and distribution of your data.\n",
        "* **Problem complexity:** The complexity of the relationship between features and the outcome.\n",
        "* **Interpretability:** Whether you need to understand how the model makes predictions.\n",
        "* **Computational resources:** The amount of computing power and time available.\n",
        "\n",
        "**It's often a good idea to try multiple algorithms and compare their performance using appropriate evaluation metrics.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaV95_z4FfCB"
      },
      "source": [
        "11. What are classification evaluation Metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9_wzRN6FpK-"
      },
      "source": [
        "ans. You're right to ask about evaluation metrics! They're essential for understanding how well your classification model is performing. Here's a breakdown of some key metrics:\n",
        "\n",
        "**1. Confusion Matrix:**\n",
        "\n",
        "* This is the foundation for many other metrics. It's a table that summarizes the model's predictions compared to the actual outcomes.\n",
        "\n",
        "|  | Predicted Positive | Predicted Negative |\n",
        "|---|---|---|\n",
        "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
        "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
        "\n",
        "**2. Accuracy:**\n",
        "\n",
        "* The most straightforward metric. It measures the overall correctness of the model's predictions.\n",
        "    * **Formula:** (TP + TN) / (TP + TN + FP + FN)\n",
        "    * **Limitation:** Can be misleading if the classes are imbalanced (e.g., many more negatives than positives).\n",
        "\n",
        "**3. Precision:**\n",
        "\n",
        "* Measures how many of the positive predictions were actually correct.\n",
        "    * **Formula:** TP / (TP + FP)\n",
        "    * **Use case:** Important when false positives are costly (e.g., spam detection).\n",
        "\n",
        "**4. Recall (Sensitivity or True Positive Rate):**\n",
        "\n",
        "* Measures how many of the actual positives were correctly identified.\n",
        "    * **Formula:** TP / (TP + FN)\n",
        "    * **Use case:** Important when false negatives are costly (e.g., disease diagnosis).\n",
        "\n",
        "**5. F1-Score:**\n",
        "\n",
        "* The harmonic mean of precision and recall. It balances both metrics.\n",
        "    * **Formula:** 2 * (Precision * Recall) / (Precision + Recall)\n",
        "    * **Use case:** Useful when you want a single metric that considers both false positives and false negatives.\n",
        "\n",
        "**6. Specificity (True Negative Rate):**\n",
        "\n",
        "* Measures how many of the actual negatives were correctly identified.\n",
        "    * **Formula:** TN / (TN + FP)\n",
        "    * **Use case:** Important when correctly identifying negatives is crucial.\n",
        "\n",
        "**7. ROC Curve and AUC:**\n",
        "\n",
        "* **ROC Curve:** A graphical representation of the model's performance at different classification thresholds.\n",
        "* **AUC (Area Under the Curve):** A single value summarizing the ROC curve. Higher AUC indicates better performance.\n",
        "* **Use case:** Useful for understanding the trade-off between true positive rate and false positive rate.\n",
        "\n",
        "**Choosing the Right Metric:**\n",
        "\n",
        "The best metric depends on your specific problem and priorities. Consider:\n",
        "\n",
        "* **Class balance:** If classes are imbalanced, accuracy might not be the best choice.\n",
        "* **Costs of errors:** Are false positives or false negatives more costly in your application?\n",
        "* **Overall goals:** What are you trying to achieve with your model?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APC4vd86FyC_"
      },
      "source": [
        "12. How does class imbalance affect Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j64QQt1pF6_R"
      },
      "source": [
        "ans. Class imbalance, where one class has significantly more examples than the other, can indeed affect Logistic Regression. Here's how:\n",
        "\n",
        "**1. Bias Towards the Majority Class:**\n",
        "\n",
        "- Logistic Regression, by default, tries to minimize the overall error rate. In imbalanced datasets, it might achieve this by simply predicting the majority class most of the time. This leads to a model that performs well on the majority class but poorly on the minority class, which is often the class of interest.\n",
        "\n",
        "**2. Misleading Accuracy:**\n",
        "\n",
        "- Accuracy, a common evaluation metric, can be misleading in imbalanced scenarios. A model that always predicts the majority class might have high accuracy, even if it completely fails to identify the minority class.\n",
        "\n",
        "**3. Difficulty in Learning Minority Class Patterns:**\n",
        "\n",
        "- With fewer examples, the model has less information to learn the patterns of the minority class. This can result in poor predictions for this class, even if the model performs well overall.\n",
        "\n",
        "**4. Impact on Coefficients:**\n",
        "\n",
        "- Class imbalance can affect the coefficients learned by the model. The coefficients might be biased towards the majority class, making it difficult to interpret their true impact on the minority class.\n",
        "\n",
        "**How to Address Class Imbalance in Logistic Regression:**\n",
        "\n",
        "Several techniques can be used to mitigate the effects of class imbalance:\n",
        "\n",
        "* **Resampling Techniques:**\n",
        "    - **Oversampling:** Increase the number of examples in the minority class by duplicating existing instances or creating synthetic samples (e.g., SMOTE).\n",
        "    - **Undersampling:** Reduce the number of examples in the majority class by randomly removing instances.\n",
        "* **Cost-Sensitive Learning:**\n",
        "    - Assign higher weights to the minority class during training, making the model pay more attention to it.\n",
        "* **Probability Threshold Adjustment:**\n",
        "    - Instead of using the default threshold of 0.5 for classification, adjust the threshold to optimize for metrics like precision, recall, or F1-score, which are more informative in imbalanced scenarios.\n",
        "* **Ensemble Methods:**\n",
        "    - Use ensemble methods like Random Forests or Gradient Boosting Machines, which are less sensitive to class imbalance.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Class imbalance can significantly impact Logistic Regression by biasing the model towards the majority class and making accuracy a misleading metric. However, several techniques can be used to address this issue and improve the model's performance on the minority class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RexeJ1VfGGnw"
      },
      "source": [
        "13.  What is hyperparameter Tuning in Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npLRUQACGV8w"
      },
      "source": [
        "ans. Hyperparameter tuning in Logistic Regression is the process of finding the best set of values for the model's hyperparameters. These are parameters that are set *before* the training process and control the learning behavior of the model. Unlike model parameters (like the coefficients), hyperparameters are not learned from the data.\n",
        "\n",
        "**Why is it important?**\n",
        "\n",
        "The performance of your Logistic Regression model can be significantly affected by the choice of hyperparameters. Tuning them correctly can lead to:\n",
        "\n",
        "* **Improved accuracy:** A well-tuned model can make more accurate predictions.\n",
        "* **Better generalization:** It can perform better on unseen data.\n",
        "* **Reduced overfitting:** It can prevent the model from memorizing noise in the training data.\n",
        "\n",
        "**Key Hyperparameters in Logistic Regression:**\n",
        "\n",
        "1. **Regularization (Penalty):**\n",
        "   - `penalty`: Specifies the type of regularization to use ('l1' for Lasso, 'l2' for Ridge, 'none' for no regularization).\n",
        "   - `C`: Inverse of regularization strength. Smaller values indicate stronger regularization.\n",
        "\n",
        "2. **Solver:**\n",
        "   - `solver`: Algorithm used for optimization (e.g., 'liblinear', 'lbfgs', 'sag'). Different solvers work better with different datasets and penalties.\n",
        "\n",
        "3. **Maximum Iterations:**\n",
        "   - `max_iter`: Maximum number of iterations for the solver to converge.\n",
        "\n",
        "**Techniques for Hyperparameter Tuning:**\n",
        "\n",
        "1. **Grid Search:**\n",
        "   - Define a grid of possible hyperparameter values.\n",
        "   - Train and evaluate the model for all combinations in the grid.\n",
        "   - Select the combination with the best performance.\n",
        "\n",
        "2. **Randomized Search:**\n",
        "   - Similar to grid search, but samples random combinations from the hyperparameter space.\n",
        "   - More efficient than grid search when the hyperparameter space is large.\n",
        "\n",
        "3. **Cross-Validation:**\n",
        "   - Used in conjunction with grid search or randomized search.\n",
        "   - Splits the data into multiple folds.\n",
        "   - Trains and evaluates the model on different combinations of folds to get a more robust estimate of performance.\n",
        "\n",
        "**Tools for Hyperparameter Tuning:**\n",
        "\n",
        "* **Scikit-learn:** Provides tools like `GridSearchCV` and `RandomizedSearchCV` for hyperparameter tuning.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Hyperparameter tuning is a crucial step in building a high-performing Logistic Regression model. By systematically exploring different hyperparameter values, you can find the optimal configuration that maximizes the model's accuracy and generalization ability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIY2uDs4GhT_"
      },
      "source": [
        "14. What are different solvers in Logistic Regression ? Which one should be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5TdzqF4Gy76"
      },
      "source": [
        "ans. You're asking a great question! The `solver` parameter in Logistic Regression (specifically in libraries like scikit-learn) determines the optimization algorithm used to find the best coefficients for your model.  Different solvers have different strengths and weaknesses, making them suitable for various situations. Here's a breakdown:\n",
        "\n",
        "**Common Solvers and Their Characteristics:**\n",
        "\n",
        "* **`liblinear`:**\n",
        "    * **Pros:** Works well with L1 and L2 regularization. Efficient for large datasets.  Good for linear problems.\n",
        "    * **Cons:** Doesn't support `penalty='none'` (no regularization). Can struggle with very complex, non-linear relationships.\n",
        "* **`lbfgs` (Limited-memory Broyden-Fletcher-Goldfarb-Shanno):**\n",
        "    * **Pros:**  A good all-around solver. Handles L2 regularization well.  Often a good default choice.  Generally faster than `newton-cg` for many problems.\n",
        "    * **Cons:** Doesn't support L1 regularization. Can be sensitive to scaling of features.\n",
        "* **`newton-cg` (Newton-Conjugate Gradient):**\n",
        "    * **Pros:**  Handles L2 regularization. Can be more accurate than `lbfgs` for some problems, especially when convergence is difficult.\n",
        "    * **Cons:** Doesn't support L1 regularization. Can be slower than `lbfgs`.\n",
        "* **`sag` (Stochastic Average Gradient):**\n",
        "    * **Pros:**  Scales well to very large datasets.  Supports L1 and L2 regularization (but L1 is often slower).\n",
        "    * **Cons:** Can be slower than `liblinear` for smaller datasets.  Convergence can be slow for some problems.\n",
        "* **`saga` (Stochastic Average Gradient - improved):**\n",
        "    * **Pros:**  Improved version of `sag`.  Supports L1 and L2 regularization.  Often converges faster and more reliably than `sag`.  Generally a good choice for large datasets with L1 or L2 regularization.\n",
        "    * **Cons:** Can still be slower than `liblinear` for smaller datasets.\n",
        "\n",
        "**Which Solver Should You Use?**\n",
        "\n",
        "The best solver depends on your specific situation:\n",
        "\n",
        "1. **Small to Medium Datasets, L1 or L2 Regularization:**  `liblinear` is often a good first choice due to its efficiency.\n",
        "\n",
        "2. **Small to Medium Datasets, L2 Regularization:** `lbfgs` is usually a strong contender and often the default.\n",
        "\n",
        "3. **Large Datasets, L1 or L2 Regularization:** `saga` is usually the best option due to its scalability. `sag` is also an option, but `saga` is generally preferred.\n",
        "\n",
        "4. **No Regularization (`penalty='none'`):**  `lbfgs`, `newton-cg`, `sag`, and `saga` are the only options. `lbfgs` is usually a good starting point.\n",
        "\n",
        "5. **Complex or Non-linear Relationships:** If you suspect a more complex relationship, `newton-cg` or `lbfgs` (with careful feature scaling) may be worth trying, but consider other models altogether if linearity is a major issue.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmrpjyW3HMVY"
      },
      "source": [
        "15. How is Logistic Regression  extended for multiclass classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tit36zkeHZrD"
      },
      "source": [
        "ans. You're right to think about how Logistic Regression can handle more than two classes! It's a common need. Here's how it's extended for multiclass classification:\n",
        "\n",
        "**1. Softmax Regression (Multinomial Logistic Regression):**\n",
        "\n",
        "* **The Idea:** This is the most common extension. Instead of the sigmoid function (which works for two classes), it uses the *softmax function*. Softmax calculates the probability of each class, and these probabilities sum up to 1.\n",
        "* **How it Works:**\n",
        "    * It learns a set of coefficients for each class.\n",
        "    * For a given data point, it calculates a score for each class using these coefficients.\n",
        "    * The softmax function converts these scores into probabilities.\n",
        "    * The class with the highest probability is assigned as the prediction.\n",
        "* **Use Case:** When the classes are mutually exclusive (an item can belong to only one class). Examples: classifying different types of fruits, categorizing news articles into topics.\n",
        "\n",
        "**2. One-vs-All (OvA) or One-vs-Rest:**\n",
        "\n",
        "* **The Idea:** This approach trains a separate Logistic Regression classifier for each class. Each classifier predicts whether a data point belongs to that specific class or not (treating all other classes as \"not this class\").\n",
        "* **How it Works:**\n",
        "    * For each class, it creates a binary classification problem (class vs. all others).\n",
        "    * It trains a Logistic Regression model on this binary problem.\n",
        "    * To make a prediction, it runs all the classifiers and chooses the class with the highest probability.\n",
        "* **Use Case:** Can be used when classes are not necessarily mutually exclusive (an item could belong to multiple categories).\n",
        "\n",
        "**3. Ordinal Logistic Regression:**\n",
        "\n",
        "* **The Idea:** This is used when the classes have an inherent order (e.g., low, medium, high). It takes this order into account when making predictions.\n",
        "* **How it Works:**\n",
        "    * It uses a different model formulation that considers the cumulative probabilities of belonging to a class or below.\n",
        "* **Use Case:** Rating scales, customer satisfaction levels, disease severity.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Method | Function | Class Relationship |\n",
        "|---|---|---|\n",
        "| Softmax Regression | Softmax | Mutually exclusive |\n",
        "| One-vs-All | Sigmoid (multiple times) | Not necessarily mutually exclusive |\n",
        "| Ordinal Logistic Regression | Specialized for ordered categories | Ordered |\n",
        "\n",
        "**Which to Use:**\n",
        "\n",
        "* **Softmax:** Most common and generally preferred for mutually exclusive classes.\n",
        "* **One-vs-All:** Useful when classes might overlap.\n",
        "* **Ordinal:** Specifically for ordered categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xImvFAIvHvx7"
      },
      "source": [
        "16. What are the advantages and disadvantage of Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTTXvN-IH-oL"
      },
      "source": [
        "ans. You're right to weigh the pros and cons! Logistic Regression is a valuable tool, but it's important to understand its limitations. Here's a breakdown:\n",
        "\n",
        "**Advantages of Logistic Regression:**\n",
        "\n",
        "* **Interpretability:** One of the biggest strengths. The coefficients of the model can be easily interpreted in terms of their impact on the outcome. This is very valuable in fields like healthcare or finance where understanding *why* a prediction is made is crucial.\n",
        "* **Efficiency:** Logistic Regression is computationally efficient, both in training and prediction. It can handle large datasets relatively quickly.\n",
        "* **Simplicity:** It's a relatively simple model to understand and implement, making it a good starting point for many classification problems.\n",
        "* **Probability Output:** It provides probability estimates for each class, which can be useful for decision-making.\n",
        "* **Handles Different Data Types:** Can handle both categorical and numerical features.\n",
        "* **Widely Available:** Implemented in most machine learning libraries.\n",
        "\n",
        "**Disadvantages of Logistic Regression:**\n",
        "\n",
        "* **Linearity Assumption:** Assumes a linear relationship between the features and the log-odds of the outcome. This assumption might not hold in many real-world scenarios, limiting its ability to capture complex relationships.\n",
        "* **Sensitivity to Outliers:** Can be sensitive to outliers in the data, which can affect the model's coefficients.\n",
        "* **Overfitting:** Prone to overfitting, especially with high-dimensional datasets. Regularization techniques are often needed to mitigate this.\n",
        "* **Limited Complexity:** Can struggle with complex, non-linear problems. More complex models like neural networks might be necessary in such cases.\n",
        "* **Multicollinearity:** Sensitive to multicollinearity (high correlation between features). This can make it difficult to interpret the individual effects of features.\n",
        "* **Class Imbalance:** Can be affected by class imbalance, where one class has significantly more examples than the other.\n",
        "\n",
        "**When to Use Logistic Regression:**\n",
        "\n",
        "* **Binary classification problems:** Where the outcome has only two possible values.\n",
        "* **When interpretability is important:** When you need to understand the relationship between features and the outcome.\n",
        "* **When you have a relatively simple dataset:** Where the relationship between features and the outcome is roughly linear.\n",
        "* **As a baseline model:** To compare the performance of more complex models.\n",
        "\n",
        "**When Not to Use Logistic Regression:**\n",
        "\n",
        "* **Non-linear relationships:** When the relationship between features and the outcome is highly non-linear.\n",
        "* **Complex problems:** When the problem requires capturing intricate patterns in the data.\n",
        "* **High-dimensional datasets with many irrelevant features:** When feature selection is crucial.\n",
        "* **When you need very high accuracy:** In some cases, more complex models might provide better accuracy.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Logistic Regression is a valuable tool for many classification tasks, especially when interpretability and efficiency are important. However, it's crucial to be aware of its limitations and consider alternative models when the assumptions of linearity and simplicity don't hold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jKP4qYgILTS"
      },
      "source": [
        "17. What are some use cases of Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09wtC8DMITqb"
      },
      "source": [
        "ans. Logistic Regression is a versatile algorithm with applications across various domains. Here are some common use cases:\n",
        "\n",
        "**Healthcare:**\n",
        "\n",
        "* **Disease Prediction:** Predicting the likelihood of a patient having a certain disease based on their symptoms and medical history.  For example, predicting the risk of diabetes, heart disease, or cancer.\n",
        "* **Patient Readmission:** Predicting the probability of a patient being readmitted to the hospital after discharge.\n",
        "* **Treatment Response:** Predicting whether a patient will respond positively to a particular treatment.\n",
        "* **Risk Assessment:** Assessing the risk of a patient developing a specific condition.\n",
        "\n",
        "**Finance:**\n",
        "\n",
        "* **Credit Scoring:** Determining the creditworthiness of loan applicants based on their financial history and demographics.\n",
        "* **Fraud Detection:** Identifying potentially fraudulent transactions.\n",
        "* **Customer Churn Prediction:** Predicting the likelihood of customers canceling their services.\n",
        "* **Market Analysis:** Analyzing market trends and predicting the success of new products.\n",
        "\n",
        "**Marketing:**\n",
        "\n",
        "* **Customer Segmentation:** Grouping customers based on their demographics, behavior, and preferences.\n",
        "* **Targeted Advertising:** Predicting which customers are most likely to respond to a particular advertisement.\n",
        "* **Lead Scoring:** Ranking leads based on their likelihood of converting into customers.\n",
        "* **Campaign Optimization:** Optimizing marketing campaigns by predicting which channels and messages are most effective.\n",
        "\n",
        "**E-commerce:**\n",
        "\n",
        "* **Product Recommendation:** Recommending products to customers based on their past purchases and browsing history.\n",
        "* **Purchase Prediction:** Predicting whether a customer will make a purchase.\n",
        "* **Customer Lifetime Value Prediction:** Predicting the total value a customer will bring to the business over their relationship.\n",
        "\n",
        "**Other Applications:**\n",
        "\n",
        "* **Spam Detection:** Classifying emails as spam or not spam.\n",
        "* **Natural Language Processing:** Sentiment analysis (determining the sentiment of a text), text classification.\n",
        "* **Image Recognition:**  In simpler image classification tasks (though deep learning is more common now).\n",
        "* **Risk Management:** Assessing the risk of various events occurring in different industries.\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "* **Binary Outcomes:** Logistic Regression is most suitable when the outcome variable is binary (two classes).\n",
        "* **Interpretability:** Its interpretability makes it valuable in situations where understanding the relationship between features and the outcome is important.\n",
        "* **Simplicity:** It's often used as a baseline model to compare the performance of more complex models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yJBVqSHIe_R"
      },
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUcPjcnlIq6I"
      },
      "source": [
        "ans. You're right to ask for the distinction! While they're closely related, there's a key difference between Softmax Regression and Logistic Regression:\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "* **Binary Classification:** It's designed for problems where the outcome has only two possible classes (e.g., yes/no, 0/1).\n",
        "* **Sigmoid Function:** It uses the sigmoid function to map a linear combination of features to a probability between 0 and 1, representing the probability of belonging to one of the two classes.\n",
        "\n",
        "**Softmax Regression:**\n",
        "\n",
        "* **Multiclass Classification:** It's a generalization of Logistic Regression for problems where the outcome can have more than two classes (e.g., classifying images of different types of animals).\n",
        "* **Softmax Function:** It uses the softmax function to map a linear combination of features to a probability distribution over all the classes. The softmax function ensures that these probabilities sum up to 1.\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature | Logistic Regression | Softmax Regression |\n",
        "|---|---|---|\n",
        "| **Number of Classes** | Two | More than two |\n",
        "| **Function** | Sigmoid | Softmax |\n",
        "| **Output** | Probability of one class (the other is implied) | Probabilities for each class |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xFeQ7_zWMXP"
      },
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8h01EgkWekm"
      },
      "source": [
        "ans. That's a great question, and the choice between One-vs-Rest (OvR) and Softmax for multiclass classification depends on the specifics of your problem. Here's a breakdown to help you decide:\n",
        "\n",
        "**One-vs-Rest (OvR):**\n",
        "\n",
        "* **How it works:** Trains a separate binary classifier for each class against all the other classes.\n",
        "* **Pros:**\n",
        "    * Simple to implement and understand.\n",
        "    * Can be used with any binary classifier, not just Logistic Regression.\n",
        "    * Works well when the number of classes is not too large.\n",
        "* **Cons:**\n",
        "    * Can suffer from \"class imbalance\" problems, as each classifier is trained on a dataset where one class is much smaller than the rest.\n",
        "    * Can lead to inconsistent probabilities, as the probabilities from different classifiers don't necessarily sum up to 1.\n",
        "\n",
        "**Softmax Regression:**\n",
        "\n",
        "* **How it works:** Directly extends Logistic Regression to handle multiple classes using the softmax function.\n",
        "* **Pros:**\n",
        "    * More elegant and unified approach.\n",
        "    * Produces well-calibrated probabilities that sum up to 1.\n",
        "    * Often more efficient than OvR, as it trains only one model.\n",
        "* **Cons:**\n",
        "    * Can only be used with models that support multiclass classification directly (like Logistic Regression).\n",
        "    * Might be less suitable when the classes are not mutually exclusive (an instance can belong to multiple classes).\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature | One-vs-Rest (OvR) | Softmax Regression |\n",
        "|---|---|---|\n",
        "| **Number of Models** | One per class | Single model |\n",
        "| **Probability Output** | Not necessarily calibrated, don't sum to 1 | Calibrated, sum to 1 |\n",
        "| **Class Imbalance** | Can be more susceptible | Less susceptible |\n",
        "| **Model Compatibility** | Works with any binary classifier | Requires native multiclass support |\n",
        "| **Efficiency** | Can be less efficient | Generally more efficient |\n",
        "\n",
        "**When to choose OvR:**\n",
        "\n",
        "* When you have a large number of classes.\n",
        "* When you want to use a binary classifier that doesn't have native multiclass support (e.g., Support Vector Machines).\n",
        "* When you don't need well-calibrated probabilities.\n",
        "\n",
        "**When to choose Softmax:**\n",
        "\n",
        "* When you have a moderate number of classes.\n",
        "* When you want well-calibrated probabilities.\n",
        "* When you're using a model that supports multiclass classification directly (e.g., Logistic Regression, Neural Networks).\n",
        "* When efficiency is a concern.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weayCYzsWtCV"
      },
      "source": [
        "20. How do we interpret coefficients in Logistic Regression ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUBi8BMlW-Y_"
      },
      "source": [
        "ans. Interpreting coefficients in Logistic Regression requires a bit of nuance, as they don't have the same straightforward interpretation as in linear regression. Here's a breakdown:\n",
        "\n",
        "**Understanding the Basics**\n",
        "\n",
        "* **Log-Odds:** Logistic Regression models the relationship between the independent variables and the *log-odds* of the outcome. The log-odds (or logit) is the natural logarithm of the odds ratio, where the odds ratio is the probability of the event occurring divided by the probability of it not occurring.\n",
        "* **Coefficients and Log-Odds:** The coefficients in Logistic Regression represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, *holding all other variables constant*.\n",
        "\n",
        "**Interpreting the Coefficients**\n",
        "\n",
        "1. **Direction:**\n",
        "   * **Positive Coefficient:** A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the log-odds of the outcome, making the event more likely.\n",
        "   * **Negative Coefficient:** A negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome, making the event less likely.\n",
        "\n",
        "2. **Magnitude:**\n",
        "   * The magnitude of the coefficient reflects the strength of the relationship between the predictor and the outcome. Larger coefficients (in absolute value) indicate a stronger effect. However, the interpretation of the magnitude is not linear, as it's in terms of log-odds.\n",
        "\n",
        "3. **Odds Ratio:**\n",
        "   * To make the interpretation more intuitive, we often convert the coefficients to *odds ratios* by exponentiating them (e.g., `exp(coefficient)`).\n",
        "   * The odds ratio represents the factor by which the odds of the outcome change for a one-unit change in the predictor.\n",
        "   * For example, an odds ratio of 2 means that a one-unit increase in the predictor doubles the odds of the outcome occurring. An odds ratio of 0.5 means that a one-unit increase in the predictor halves the odds of the outcome occurring.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Let's say we're predicting the probability of a customer clicking on an ad based on their age. The Logistic Regression model gives us a coefficient of 0.1 for the age variable.\n",
        "\n",
        "* **Log-Odds Interpretation:** For every one-year increase in age, the log-odds of clicking on the ad increase by 0.1.\n",
        "* **Odds Ratio Interpretation:** `exp(0.1) ≈ 1.105`. For every one-year increase in age, the odds of clicking on the ad increase by approximately 1.105 times (or by 10.5%).\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Units:** The interpretation depends on the units of the predictor variable.\n",
        "* **Scaling:** If the predictor variables are scaled (e.g., standardized), the interpretation will be in terms of standard deviations.\n",
        "* **Ceteris Paribus:** The interpretation assumes that all other variables in the model are held constant.\n",
        "* **Statistical Significance:** It's essential to consider the statistical significance of the coefficients (p-values) to determine if the observed relationships are likely to be real or due to chance.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Interpreting coefficients in Logistic Regression involves understanding their relationship with log-odds and converting them to odds ratios for more intuitive interpretation. The direction and magnitude of the coefficients, along with their statistical significance, provide insights into the relationship between the predictor variables and the probability of the outcome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO7DWUpIXUKE"
      },
      "source": [
        "PRACTICAL QUESTIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv-LEfFkXX-F"
      },
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxKONR44Ymbb",
        "outputId": "4d835e22-c13d-401b-93bc-0a75edf7fb19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZJr2tTLYtbP"
      },
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIsO65EmY9DE",
        "outputId": "bf3665ac-0ba1-42f8-b027-c9ca5c860f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI7tC2xDZgqm"
      },
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSZAxQnMZ43t",
        "outputId": "c321aad2-fbfe-4af3-e55a-d831e8e999cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy with L2 Regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings related to solver compatibility\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TvyxdN5aOgg"
      },
      "source": [
        "4. Write a Python program to train Logistic Regression with elastic Net Regularization (penalty='elasticnet')?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iZWo7v8acYo",
        "outputId": "7ef2796a-ea8a-4c86-a33d-415c6ca32e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8550\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85        93\n",
            "           1       0.91      0.80      0.86       107\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.86      0.86      0.85       200\n",
            "weighted avg       0.86      0.85      0.86       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model with elastic net regularization\n",
        "log_reg = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dxliTfca5HZ"
      },
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUzdc0gHbO3_",
        "outputId": "ddbd0864-0e80-482c-e568-31e38fdc6b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8550\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85        93\n",
            "           1       0.91      0.80      0.86       107\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.86      0.86      0.85       200\n",
            "weighted avg       0.86      0.85      0.86       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model with elastic net regularization\n",
        "log_reg = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzZe6pCabSuY"
      },
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy ?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))  # 'liblinear' supports l1 and l2 penalty\n",
        "])\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'logreg__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMPrpXB1_0wd",
        "outputId": "116d0432-752d-4d74-eb60-3219c9d8f60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logreg__C': 10, 'logreg__penalty': 'l1'}\n",
            "Best Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-fold cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "Cxk5mleO_-zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Define Stratified K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross-validation\n",
        "scores = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print average accuracy\n",
        "print(\"Average Accuracy:\", np.mean(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSg74NtrAc9_",
        "outputId": "0ccfc7ef-bbeb-457d-c349-2c7508025d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data as a list of lists\n",
        "data = [\n",
        "    [5.1, 3.5, 1.4, 0.2, 0],\n",
        "    [4.9, 3.0, 1.4, 0.2, 0],\n",
        "    [6.2, 3.4, 5.4, 2.3, 1],\n",
        "    [5.9, 3.0, 5.1, 1.8, 1],\n",
        "    [5.0, 3.6, 1.4, 0.2, 0],\n",
        "    [6.7, 3.1, 4.7, 1.5, 1],\n",
        "    [6.3, 2.9, 5.6, 1.8, 1],\n",
        "    [5.8, 2.7, 4.1, 1.0, 1],\n",
        "    [7.1, 3.0, 5.9, 2.1, 1],\n",
        "    [6.3, 2.5, 5.0, 1.9, 1]\n",
        "]\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['feature1', 'feature2', 'feature3', 'feature4', 'target'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('my_data.csv', index=False)  # index=False prevents writing row indices to the CSV\n",
        "\n",
        "print(\"CSV file 'my_data.csv' created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoNmhMOCGlNf",
        "outputId": "37ccb9e5-7807-481a-854d-abc02ce0be02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'my_data.csv' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy"
      ],
      "metadata": {
        "id": "TLkC3fsJAz3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file\n",
        "df = pd.read_csv('my_data.csv')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "y = df.iloc[:, -1]\n",
        "X = df.iloc[:, :-1]\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgtb42fiA_Qv",
        "outputId": "e960e343-82a8-4782-ec7e-5e3a2fba1466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "tfhXn-CnG2lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset from CSV file\n",
        "df = pd.read_csv('my_data.csv')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "y = df.iloc[:, -1]\n",
        "X = df.iloc[:, :-1]\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Define parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'logreg__C': np.logspace(-4, 4, 10),\n",
        "    'logreg__penalty': ['l1', 'l2'],\n",
        "    'logreg__solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsNF-3ZJHjWW",
        "outputId": "bad94ffe-f99f-4861-9a24-30eb67a620c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logreg__solver': 'saga', 'logreg__penalty': 'l2', 'logreg__C': 0.3593813663804626}\n",
            "Best Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "qWf7OY14ICwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_digits()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and One-vs-One Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ovo_logreg', OneVsOneClassifier(LogisticRegression(solver='liblinear')))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMouT1wFIYUR",
        "outputId": "7916f1dd-5a77-4806-c5a7-1d6886d16284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "l_VfdtpWJT-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "RFv5ekdFjAZR",
        "outputId": "9958637a-b24f-48b1-eb1b-17eb5996e12b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "isaA_MzbjDdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoTK31Y3jf57",
        "outputId": "adf0072b-32d6-4b2d-bcd8-1ea7aad09926"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n",
            "Precision: 0.9722222222222222\n",
            "Recall: 0.9859154929577465\n",
            "F1-Score: 0.9790209790209791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. write a Python program to train a logistic Regression model on imbalalced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "6mnYX1ldj2GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression with class weights\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear', class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL_jHmIvkT89",
        "outputId": "53fb0e9e-dd16-43c1-ddf2-d6390b1a0c25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.865\n",
            "Precision: 0.40540540540540543\n",
            "Recall: 0.75\n",
            "F1-Score: 0.5263157894736842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "tPeXd4wxkxZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "df = df[features + ['Survived']].copy()\n",
        "\n",
        "# Handle missing values\n",
        "df.loc[:, 'Age'] = df['Age'].fillna(df['Age'].median())\n",
        "df.loc[:, 'Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "\n",
        "# Convert categorical variables\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with imputation, scaling, and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URcMqxWVmUDY",
        "outputId": "dbe6c80d-f2ec-4a49-fd5b-1d5798ac1073"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8100558659217877\n",
            "Precision: 0.7857142857142857\n",
            "Recall: 0.7432432432432432\n",
            "F1-Score: 0.7638888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply featuee scaling (Standardization) before training a logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "hbw-JwhSmlUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model WITHOUT scaling\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_no_scaling = logreg.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=10000))\n",
        "])\n",
        "\n",
        "# Train the model WITH scaling\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_scaling = pipeline.predict(X_test)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred_scaling)\n",
        "recall = recall_score(y_test, y_pred_scaling)\n",
        "f1 = f1_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaling)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_scaling)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyFEIJI1m9aO",
        "outputId": "5104a2f1-baff-4f67-fbd6-dd4ba46701ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling: 0.9736842105263158\n",
            "Precision: 0.9722222222222222\n",
            "Recall: 0.9859154929577465\n",
            "F1-Score: 0.9790209790209791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "Nx0PuXD5r-oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model WITHOUT scaling\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_no_scaling = logreg.predict(X_test)\n",
        "y_prob_no_scaling = logreg.predict_proba(X_test)[:, 1]\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "roc_auc_no_scaling = roc_auc_score(y_test, y_prob_no_scaling)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=10000))\n",
        "])\n",
        "\n",
        "# Train the model WITH scaling\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_scaling = pipeline.predict(X_test)\n",
        "y_prob_scaling = pipeline.predict_proba(X_test)[:, 1]\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "roc_auc_scaling = roc_auc_score(y_test, y_prob_scaling)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred_scaling)\n",
        "recall = recall_score(y_test, y_pred_scaling)\n",
        "f1 = f1_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"ROC-AUC without Scaling:\", roc_auc_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaling)\n",
        "print(\"ROC-AUC with Scaling:\", roc_auc_scaling)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_scaling)\n",
        "\n",
        "\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob_scaling)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_scaling:.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "p6rPpYVxsQ-5",
        "outputId": "c622780c-5286-4756-fa7e-d566e916e051"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "ROC-AUC without Scaling: 0.9977071732721913\n",
            "Accuracy with Scaling: 0.9736842105263158\n",
            "ROC-AUC with Scaling: 0.99737962659679\n",
            "Precision: 0.9722222222222222\n",
            "Recall: 0.9859154929577465\n",
            "F1-Score: 0.9790209790209791\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train LoGistic Regression using a custom learning rate (C = 0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "_i9Mmg1oshZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model WITHOUT scaling\n",
        "logreg = LogisticRegression(C=0.5, max_iter=10000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_no_scaling = logreg.predict(X_test)\n",
        "y_prob_no_scaling = logreg.predict_proba(X_test)[:, 1]\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "roc_auc_no_scaling = roc_auc_score(y_test, y_prob_no_scaling)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(C=0.5, max_iter=10000))\n",
        "])\n",
        "\n",
        "# Train the model WITH scaling\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_scaling = pipeline.predict(X_test)\n",
        "y_prob_scaling = pipeline.predict_proba(X_test)[:, 1]\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "roc_auc_scaling = roc_auc_score(y_test, y_prob_scaling)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred_scaling)\n",
        "recall = recall_score(y_test, y_pred_scaling)\n",
        "f1 = f1_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"ROC-AUC without Scaling:\", roc_auc_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaling)\n",
        "print(\"ROC-AUC with Scaling:\", roc_auc_scaling)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_scaling)\n",
        "\n",
        "\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob_scaling)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_scaling:.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "umG5h2QLs4ot",
        "outputId": "ba6ea27b-c877-4779-b55f-d6a71261392d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.9649122807017544\n",
            "ROC-AUC without Scaling: 0.99737962659679\n",
            "Accuracy with Scaling: 0.9736842105263158\n",
            "ROC-AUC with Scaling: 0.99737962659679\n",
            "Precision: 0.9722222222222222\n",
            "Recall: 0.9859154929577465\n",
            "F1-Score: 0.9790209790209791\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train logistic Regression and identify important features based on model\n",
        "coefficients"
      ],
      "metadata": {
        "id": "EfiVcJahtkXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Extract feature importance from model coefficients\n",
        "feature_importance = np.abs(model.coef_[0])\n",
        "important_features = sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display top 10 most important features\n",
        "print(\"\\nTop 10 Important Features:\")\n",
        "for feature, importance in important_features[:10]:\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODVggGH0t0Cf",
        "outputId": "a1e8f93c-bf38-4781-928c-3b1efcc96dcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9737\n",
            "\n",
            "Top 10 Important Features:\n",
            "worst texture: 1.3506\n",
            "radius error: 1.2682\n",
            "worst symmetry: 1.2082\n",
            "mean concave points: 1.1198\n",
            "worst concavity: 0.9431\n",
            "area error: 0.9072\n",
            "worst radius: 0.8798\n",
            "worst area: 0.8418\n",
            "mean concavity: 0.8015\n",
            "worst concave points: 0.7782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "9cfOxSs1uPs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXEmzh7hulrL",
        "outputId": "61044591-26a4-41ae-d2f2-0b36e6ca9442"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9825\n",
            "Cohen's Kappa Score: 0.9623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "STASNiKNuwYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get prediction probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label=f'Logistic Regression (AP={avg_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "gAfWxfmCvCqm",
        "outputId": "2ad28639-a35e-496c-9bb6-f19acba1707f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "_XdJzaNFvREF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracy_results = {}\n",
        "\n",
        "# Train and evaluate models using different solvers\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = accuracy\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Display comparison results\n",
        "print(\"\\nComparison of Different Solvers:\")\n",
        "for solver, acc in accuracy_results.items():\n",
        "    print(f\"{solver}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEzWEs8CvlLl",
        "outputId": "3bd7c9eb-5797-4467-9aeb-4495221298a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.9825\n",
            "Solver: saga, Accuracy: 0.9825\n",
            "Solver: lbfgs, Accuracy: 0.9825\n",
            "\n",
            "Comparison of Different Solvers:\n",
            "liblinear: 0.9825\n",
            "saga: 0.9825\n",
            "lbfgs: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "NVC__Qc6v4SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate Matthews Correlation Coefficient (MCC)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi0SwPINwLOG",
        "outputId": "0f8cc7b6-e6a6-499c-b340-a320709e434f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9825\n",
            "Matthews Correlation Coefficient (MCC): 0.9623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. write a Python program to train logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "Q3UBLieLwUmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train logistic regression on raw data (without scaling) with increased iterations\n",
        "model_raw = LogisticRegression(max_iter=5000, solver='saga', random_state=42)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "# Train logistic regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare results\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Display conclusion\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"\\nFeature scaling improved model performance.\")\n",
        "elif accuracy_scaled < accuracy_raw:\n",
        "    print(\"\\nFeature scaling reduced model performance.\")\n",
        "else:\n",
        "    print(\"\\nFeature scaling had no significant impact on model performance.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW4dQ44wxccU",
        "outputId": "0878ae12-9ff3-4955-f331-af35de65247a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 0.9474\n",
            "Accuracy on Standardized Data: 0.9825\n",
            "\n",
            "Feature scaling improved model performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. write a Python program to train logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation\n"
      ],
      "metadata": {
        "id": "U_qt99y2xidh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid for C (regularization strength)\n",
        "param_grid = {'C': np.logspace(-4, 4, 10)}  # C values from 1e-4 to 1e4\n",
        "\n",
        "# Perform grid search with cross-validation (5-fold)\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000, solver='liblinear', random_state=42),\n",
        "                           param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best C value\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Optimal C (regularization strength): {best_C:.4f}\")\n",
        "\n",
        "# Train final model with the best C\n",
        "best_model = LogisticRegression(C=best_C, max_iter=1000, solver='liblinear', random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Optimal C: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCoVzvbxx30F",
        "outputId": "61babe16-e015-46fc-c0fd-a8e28900342e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C (regularization strength): 0.0464\n",
            "Test Accuracy with Optimal C: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "ud2hMrDayJdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib  # Import joblib for saving and loading models\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Predict using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy of Loaded Model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5VvxxEayhUG",
        "outputId": "09fa0e8d-016e-4ef0-a126-d325f4d5f58f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Model loaded successfully!\n",
            "Test Accuracy of Loaded Model: 0.9825\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}