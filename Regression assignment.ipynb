{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7120da4a-cb87-4f5c-8990-0a1f2b78ca21",
   "metadata": {},
   "source": [
    "**1)  What is Simple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc35bf-3b56-4072-980b-ca3fcd6c4bb4",
   "metadata": {},
   "source": [
    "ans. Simple Linear Regression :\n",
    "\n",
    "Simple linear regression is a statistical method used to model the relationship between two continuous variables. It aims to find a linear equation that best describes the correlation between an independent variable (predictor) and a dependent variable (response).\n",
    "\n",
    "* Key Concepts:\n",
    "\n",
    "i) Independent Variable (x): The variable that is believed to influence the dependent variable.\n",
    "\n",
    "ii)Dependent Variable (y): The variable that is being predicted or explained by the independent variable.\n",
    "\n",
    "iii) Linear Relationship: The relationship between the two variables is assumed to be linear, meaning it can be represented by a straight line.\n",
    "\n",
    "iv) Regression Line: The line that best fits the data points, representing the predicted relationship between the variables.\n",
    "\n",
    "v) Equation: The equation of the regression line is typically expressed as:\n",
    "  y = a + bx, where:\n",
    "    \n",
    "    a) y is the predicted value of the dependent variable\n",
    "    \n",
    "    b) a is the intercept (the value of y when x is 0)\n",
    "    \n",
    "    c) b is the slope (the change in y for a unit change in x)\n",
    "    \n",
    "    d) x is the independent variable\n",
    "\n",
    "* Applications:\n",
    "\n",
    "i) Prediction: Predicting the value of the dependent variable for a given value of the independent variable.\n",
    "\n",
    "ii) Understanding Relationships: Determining the strength and direction of the relationship between two variables.\n",
    "\n",
    "iii) Model Building: Creating a simple model to explain the relationship between variables.\n",
    "\n",
    "* Example:\n",
    "\n",
    "Consider a study investigating the relationship between hours of study and exam scores. Simple linear regression could be used to model this relationship, with hours of study as the independent variable and exam score as the dependent variable. The regression line would represent the predicted exam score for a given number of study hours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09baec67-1363-4275-a290-7d0a0538888f",
   "metadata": {},
   "source": [
    "**2) What are the key assumptions of Simple Linear Regression ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e3ee8-6e45-4133-82b0-7206212c4c20",
   "metadata": {},
   "source": [
    "ans. The key assumptions of Simple Linear Regression are:\n",
    "\n",
    "1. Linearity: \n",
    "   - The relationship between the independent variable (X) and the mean of the dependent variable (Y) must be linear. This means the data points should roughly form a straight line when plotted on a scatterplot.\n",
    "\n",
    "2. Independence of Errors:\n",
    "   - The errors (residuals) should be independent of each other. This means that the error in one observation should not be related to the error in another observation.\n",
    "\n",
    "3. Homoscedasticity (Constant Variance): \n",
    "   - The variance of the errors should be constant for all values of the independent variable (X). In other words, the spread of the data points around the regression line should be roughly equal across the range of X values.\n",
    "\n",
    "4. Normality of Errors:\n",
    "   - The errors should be normally distributed for any given value of the independent variable (X). This means that the distribution of the residuals should roughly follow a bell-shaped curve.\n",
    "\n",
    "These assumptions are important because they ensure that the results of the regression analysis are reliable and meaningful. If these assumptions are violated, the regression model may not accurately reflect the true relationship between the variables, and the predictions made by the model may be inaccurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e458f0-c130-4193-88fd-6ec8587d5de0",
   "metadata": {},
   "source": [
    "**3)  What does the coefficient m represent in the equation Y=mX+c ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd116c8-d306-4f77-a9a4-e1053ad7015c",
   "metadata": {},
   "source": [
    "ans. In the equation Y = mX + c, the coefficient 'm' represents the slope of the line. \n",
    "\n",
    "Here's what that means:\n",
    "\n",
    "* Steepness: The slope determines how steeply the line inclines or declines. \n",
    "\n",
    "\n",
    "  a) A positive 'm' indicates an upward-sloping line (as X increases, Y increases).\n",
    "\n",
    "\n",
    "   b) A negative 'm' indicates a downward-sloping line (as X increases, Y decreases).\n",
    "\n",
    "\n",
    "   c) If 'm' is zero, the line is horizontal (no change in Y as X changes).\n",
    "\n",
    "* Rate of Change: The slope represents the rate at which Y changes for a unit change in X. For example, if m = 2, then for every 1 unit increase in X, Y increases by 2 units.\n",
    "\n",
    "* In simpler terms: 'm' tells you how much Y changes for each step you take along the X-axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc76610-3aa3-42b9-9e39-cf05db382133",
   "metadata": {},
   "source": [
    "4) What does the intercept c represent in the equation Y=mX+c ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e07aff-2c9d-4e2e-8a29-14f02ebf8310",
   "metadata": {},
   "source": [
    "ans.  In the equation Y = mX + c, the coefficient 'c' represents the y-intercept of the line. \n",
    "\n",
    "Here's what that means:\n",
    "\n",
    "* Point of Intersection: The y-intercept is the point where the line crosses the y-axis. \n",
    "* Value of Y when X is Zero: When X is 0, the term 'mX' becomes zero, and the equation simplifies to Y = c. This means 'c' is the value of Y when X is 0.\n",
    "\n",
    "* In simpler terms: 'c' tells you where the line starts (or intersects) on the y-axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfdd3a9-a0d5-4fea-ad6b-be395c077129",
   "metadata": {},
   "source": [
    "**5) How do we calculate the slope m in Simple Linear Regression ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a8228-a70b-492b-a978-cb669dcd84af",
   "metadata": {},
   "source": [
    "ans. The slope (m) in simple linear regression represents the rate of change in the dependent variable (Y) for a unit change in the independent variable (X). It's calculated using the following formula:\n",
    "\n",
    "**m = (n * Σ(xy) - Σ(x) * Σ(y)) / (n * Σ(x^2) - (Σ(x))^2)**\n",
    "\n",
    "where:\n",
    "\n",
    "* n = number of data points\n",
    "* Σ(xy) = sum of the product of each x and y value\n",
    "* Σ(x) = sum of all x values\n",
    "* Σ(y) = sum of all y values\n",
    "* Σ(x^2) = sum of the squares of all x values\n",
    "\n",
    "**Steps to Calculate the Slope:**\n",
    "\n",
    "1. **Calculate the sums:**\n",
    "   - Σ(x), Σ(y), Σ(x^2), and Σ(xy)\n",
    "\n",
    "2. **Substitute the values** into the formula.\n",
    "\n",
    "3. **Calculate the slope (m)**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say we have the following data:\n",
    "\n",
    "| X | Y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 4 |\n",
    "| 3 | 5 |\n",
    "| 4 | 4 |\n",
    "| 5 | 7 |\n",
    "\n",
    "First, calculate the sums:\n",
    "\n",
    "* Σ(x) = 1 + 2 + 3 + 4 + 5 = 15\n",
    "* Σ(y) = 2 + 4 + 5 + 4 + 7 = 22\n",
    "* Σ(x^2) = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55\n",
    "* Σ(xy) = (1 * 2) + (2 * 4) + (3 * 5) + (4 * 4) + (5 * 7) = 70\n",
    "\n",
    "Now, substitute the values into the formula:\n",
    "\n",
    "m = (5 * 70 - 15 * 22) / (5 * 55 - 15^2)\n",
    "m = (350 - 330) / (275 - 225)\n",
    "m = 20 / 50\n",
    "m = 0.4\n",
    "\n",
    "Therefore, the slope of the regression line is 0.4. This means that for every 1 unit increase in X, Y is expected to increase by 0.4 units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd138122-61d4-4d92-9710-002f6b0c1b97",
   "metadata": {},
   "source": [
    "**6) What is the purpose of the least squares method in Simple Linear Regression ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd186e79-013d-4132-8f7f-8fdaeae24ce0",
   "metadata": {},
   "source": [
    "ans. The purpose of the least squares method in Simple Linear Regression is to find the **line of best fit** that minimizes the sum of the squared differences between the actual data points and the predicted values on the regression line. \n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "i) Minimizing Errors:\n",
    "    - The least squares method aims to find the values for the slope (m) and intercept (c) in the equation Y = mX + c that minimize the sum of the squared vertical distances between each data point and the corresponding point on the regression line. These vertical distances are called residuals.\n",
    "\n",
    "ii) Best Fit:\n",
    "     -By minimizing these squared residuals, the least squares method determines the line that most closely represents the overall trend in the data. This line provides the best estimate of the relationship between the independent variable (X) and the dependent variable (Y).\n",
    "\n",
    "iii) Predictions:\n",
    "       -The resulting regression line can then be used to make predictions about the value of Y for new values of X.\n",
    "\n",
    "iv) In essence, the least squares method helps us find the most accurate and representative linear model for the given data.\n",
    "\n",
    "Let me know if you'd like a more visual explanation or have further questions! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee37a94-f109-4d24-9e44-79bef0a8df6e",
   "metadata": {},
   "source": [
    "**7) How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f6e7f-950f-4e02-8e43-e75a651e9b41",
   "metadata": {},
   "source": [
    "ans. The coefficient of determination (R²) in simple linear regression is a statistical measure that represents the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X). \n",
    "\n",
    "* Key Interpretations:\n",
    "\n",
    "* Goodness of Fit: R² indicates how well the regression model fits the observed data. \n",
    "    * A higher R² suggests a better fit, meaning the model explains a larger portion of the variability in Y.\n",
    "* Explained Variation: It quantifies the proportion of the total variation in Y that is explained by the linear relationship with X.\n",
    "* Strength of Association: R² provides an estimate of the strength of the linear association between X and Y.\n",
    "\n",
    "* Range:\n",
    "* R² values range from 0 to 1.\n",
    "    * R² = 0: The model explains none of the variability in Y.\n",
    "    * R² = 1: The model perfectly explains all the variability in Y.\n",
    "\n",
    "* Example:\n",
    " If R² is 0.75, it means that 75% of the variation in Y can be explained by the linear relationship with X. The remaining 25% of the variation remains unexplained.\n",
    "\n",
    "*Important Considerations:\n",
    "\n",
    "i) While a higher R² generally indicates a better fit, it doesn't guarantee that the model is a good representation of the underlying relationship or that it will make accurate predictions.\n",
    "\n",
    "ii) R² can be influenced by various factors, including the number of data points, the presence of outliers, and the complexity of the relationship between the variables.\n",
    "\n",
    "iii) It's essential to consider other factors, such as the significance of the regression coefficients and the assumptions of the model, when evaluating the overall quality of the regression analysis.\n",
    "\n",
    "By understanding the coefficient of determination, you can better assess the quality of your simple linear regression model and interpret its results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a675a-3472-43aa-a913-2d0d2d8ebc1c",
   "metadata": {},
   "source": [
    "**8) What is Multiple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cb6c7-1b85-4226-88fc-7d82666b8ed7",
   "metadata": {},
   "source": [
    "ans. Multiple Linear Regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables. It extends the concept of simple linear regression, which only considers one independent variable.\n",
    "\n",
    "* Key Concepts:\n",
    "\n",
    "i) Dependent Variable (Y): The variable you want to predict or explain.\n",
    "\n",
    "ii)Independent Variables (X1, X2, X3, ...): The variables that are believed to influence the dependent variable.\n",
    "\n",
    "iii) Linear Relationship: The model assumes a linear relationship between the dependent variable and each independent variable.\n",
    "\n",
    "iv) Equation:\n",
    "   a) Y = b0 + b1X1 + b2X2 + ... + bnXn + ε\n",
    "    \n",
    "    b) Y: Predicted value of the dependent variable\n",
    "    \n",
    "    c)b0: Intercept (value of Y when all independent variables are 0)\n",
    "     \n",
    "    d) b1, b2, ..., bn: Coefficients representing the change in Y for a unit change in each independent variable, holding other variables constant\n",
    "     \n",
    "    e)X1, X2, ..., Xn: Independent variables\n",
    "     \n",
    "    f) ε: Error term (residual)\n",
    "\n",
    "* Applications: \n",
    "\n",
    "a) Prediction: Predicting the value of the dependent variable based on the values of multiple independent variables.\n",
    "\n",
    "b) Understanding Relationships: Determining the strength and direction of the relationship between the dependent variable and each independent variable, while controlling for the effects of other variables.\n",
    "\n",
    "c) Model Building: Creating more complex models to explain real-world phenomena.\n",
    "\n",
    "* Example: \n",
    "\n",
    "Predicting house prices (dependent variable) based on factors like size, number of bedrooms, location, and age (independent variables).\n",
    "\n",
    "* Assumptions:\n",
    "\n",
    "a) Linearity: Linear relationship between the dependent variable and each independent variable.\n",
    "\n",
    "b) Independence of Errors: Errors are independent of each other.\n",
    "\n",
    "c) Homoscedasticity: The variance of the errors is constant across all values of the independent variables.\n",
    "\n",
    "d) Normality of Errors: The errors are normally distributed.\n",
    "\n",
    "e) No Multicollinearity: Independent variables are not highly correlated with each other.\n",
    "\n",
    "\n",
    "* Key Points to Remember:\n",
    "\n",
    "a) Multiple linear regression is a powerful tool for analyzing complex relationships between variables.\n",
    "\n",
    "b) It allows you to consider the combined effects of multiple factors on the dependent variable.\n",
    "\n",
    "c) It's important to check the assumptions of the model and address any violations before interpreting the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f2ec7-cd35-4697-b3af-30fb9f1c577b",
   "metadata": {},
   "source": [
    "**9) What is the main difference between Simple and Multiple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329d483-c995-4285-9b94-b0dcb7cfe8aa",
   "metadata": {},
   "source": [
    "ans. The **main difference** between Simple Linear Regression and Multiple Linear Regression lies in the **number of independent variables**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Simple Linear Regression**:\n",
    "- **Definition**: Models the relationship between one **dependent variable** (\\(Y\\)) and one **independent variable** (\\(X\\)).\n",
    "- **Equation**:\n",
    "  \\[\n",
    "  Y = mX + c\n",
    "  \\]\n",
    "  or equivalently:\n",
    "  \\[\n",
    "  Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "  \\]\n",
    "- **Purpose**: To find the line of best fit that explains the variation in \\(Y\\) using a single predictor \\(X\\).\n",
    "- **Example**: Predicting house price (\\(Y\\)) based on square footage (\\(X\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Multiple Linear Regression**:\n",
    "- **Definition**: Models the relationship between one **dependent variable** (\\(Y\\)) and two or more **independent variables** (\\(X_1, X_2, \\dots, X_p\\)).\n",
    "- **Equation**:\n",
    "  \\[\n",
    "  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\n",
    "  \\]\n",
    "- **Purpose**: To assess how multiple predictors together explain or predict the dependent variable.\n",
    "- **Example**: Predicting house price (\\(Y\\)) based on square footage (\\(X_1\\)), number of bedrooms (\\(X_2\\)), and age of the house (\\(X_3\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature                       | Simple Linear Regression              | Multiple Linear Regression            |\n",
    "|-------------------------------|---------------------------------------|---------------------------------------|\n",
    "| **Number of Independent Variables** | One (\\(X\\))                            | Two or more (\\(X_1, X_2, \\dots\\))      |\n",
    "| **Equation**                  | \\(Y = mX + c\\)                        | \\(Y = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\\) |\n",
    "| **Complexity**                | Simpler, easier to interpret          | More complex, requires additional checks (e.g., multicollinearity) |\n",
    "| **Interpretation of Coefficients** | Only one slope (\\(m\\)) to interpret      | Multiple slopes (\\(\\beta_1, \\beta_2, \\dots\\)) explain the partial effects of each predictor |\n",
    "| **Application**               | Used when only one predictor is relevant | Used when multiple factors influence \\(Y\\) |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- Simple Linear Regression is ideal for exploring the effect of a single independent variable.\n",
    "- Multiple Linear Regression is suitable when multiple variables influence the dependent variable, providing a more comprehensive model but requiring more careful interpretation and diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2a537-3484-470b-bce1-a12f8d615299",
   "metadata": {},
   "source": [
    "**10) What are the key assumptions of Multiple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930b81d-2d08-4e87-b99f-a7b39be8aeba",
   "metadata": {},
   "source": [
    "ans. Multiple Linear Regression, like its simpler counterpart, relies on several key assumptions to ensure valid and reliable results. These assumptions are:\n",
    "\n",
    "1. **Linearity:** \n",
    "   - This assumption posits a linear relationship between the dependent variable and each of the independent variables. \n",
    "   - This means that the change in the dependent variable should be roughly proportional to changes in the independent variables.\n",
    "   - Violations of linearity can lead to biased and inefficient estimates.\n",
    "\n",
    "2. **No Multicollinearity:**\n",
    "   - Multicollinearity occurs when two or more independent variables are highly correlated with each other. \n",
    "   - This can make it difficult to determine the unique contribution of each variable to the model and can inflate the standard errors of the coefficients, making it harder to draw meaningful conclusions.\n",
    "\n",
    "3. **Homoscedasticity:**\n",
    "   - This assumption states that the variance of the errors (residuals) is constant across all levels of the independent variables. \n",
    "   - In simpler terms, the spread of the data points around the regression line should be roughly equal for all values of the independent variables.\n",
    "   - Heteroscedasticity (non-constant variance) can lead to inefficient and biased estimates.\n",
    "\n",
    "4. **Independence of Errors:**\n",
    "   - The errors (residuals) should be independent of each other. \n",
    "   - This means that the error in one observation should not be related to the error in another observation.\n",
    "   - Violations of this assumption can occur in time series data or when observations are clustered.\n",
    "\n",
    "5. **Normality of Errors:**\n",
    "   - The errors (residuals) should be normally distributed. \n",
    "   - This assumption is important for hypothesis testing and constructing confidence intervals.\n",
    "\n",
    "**Consequences of Violated Assumptions:**\n",
    "\n",
    "- **Biased or inefficient estimates:** The coefficients may not accurately reflect the true relationship between the variables.\n",
    "- **Inaccurate predictions:** The model may not accurately predict future values of the dependent variable.\n",
    "- **Invalid statistical inferences:** Hypothesis tests and confidence intervals may be unreliable.\n",
    "\n",
    "**Checking Assumptions:**\n",
    "\n",
    "- **Linearity:** Scatter plots of the dependent variable against each independent variable can help visualize linearity.\n",
    "- **Multicollinearity:** Variance Inflation Factor (VIF) can be used to detect multicollinearity.\n",
    "- **Homoscedasticity:** Residual plots can be used to check for patterns in the variance of the residuals.\n",
    "- **Independence of Errors:** Durbin-Watson test can be used to check for autocorrelation in the errors.\n",
    "- **Normality of Errors:** Histograms and Q-Q plots of the residuals can be used to assess normality.\n",
    "\n",
    "By understanding and checking these assumptions, you can ensure the validity and reliability of your multiple linear regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c9fa3-35ce-4851-a64b-cea0bc82e865",
   "metadata": {},
   "source": [
    "**11)  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9a47a-fccc-41de-9767-cf2a4900346f",
   "metadata": {},
   "source": [
    "ans. **Heteroscedasticity** in multiple linear regression refers to the situation where the variance of the errors (residuals) is not constant across all levels of the independent variables. In simpler terms, the spread of the data points around the regression line is not uniform.\n",
    "\n",
    "**How it Affects Multiple Linear Regression Results:**\n",
    "\n",
    "1. **Biased Standard Errors:** Heteroscedasticity leads to biased standard errors of the regression coefficients. This means the estimates of how uncertain our coefficient estimates are will be incorrect. \n",
    "\n",
    "2. **Inaccurate Hypothesis Tests:** Biased standard errors directly impact hypothesis tests. We might incorrectly conclude that a coefficient is statistically significant (or insignificant) when it actually isn't.\n",
    "\n",
    "3. **Inefficient Estimates:** The regression coefficients themselves may not be the most efficient estimates possible. This means we might not be getting the best possible fit to the data.\n",
    "\n",
    "**Visualizing Heteroscedasticity:**\n",
    "\n",
    "* **Residual Plots:** A common way to detect heteroscedasticity is by examining a plot of the residuals against the fitted values or against each of the independent variables. If the spread of the residuals increases or decreases systematically as the values of the fitted values or independent variables change, it suggests heteroscedasticity. \n",
    "    * **Cone Shape:** A common pattern is a \"cone\" or \"fan\" shape, where the spread of the residuals increases as the fitted values increase.\n",
    "\n",
    "**Addressing Heteroscedasticity:**\n",
    "\n",
    "* **Transformations:** Transforming the dependent variable (e.g., taking the logarithm) can sometimes stabilize the variance.\n",
    "* **Weighted Least Squares (WLS):** This technique assigns weights to observations based on their variance, giving more weight to observations with smaller variances.\n",
    "* **Robust Standard Errors:** These standard errors are less sensitive to heteroscedasticity and can provide more reliable inference.\n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "Heteroscedasticity is a violation of a key assumption in multiple linear regression. It's crucial to check for heteroscedasticity and address it appropriately to ensure the validity and reliability of your regression model's results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f03dc4-c463-4b58-952e-95337670e25b",
   "metadata": {},
   "source": [
    "**12) How can you improve a Multiple Linear Regression model with high multicollinearity?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41c435-26f5-4ffe-8d6e-b944f4078e94",
   "metadata": {},
   "source": [
    "ans. **1. Identify and Quantify Multicollinearity**\n",
    "\n",
    "* **Correlation Matrix:** Calculate the correlation coefficients between all pairs of independent variables. High correlations (e.g., above 0.7 or 0.8) suggest potential multicollinearity.\n",
    "* **Variance Inflation Factor (VIF):** VIF measures how much the variance of an estimated regression coefficient is inflated due to correlations with other predictors. \n",
    "    * A VIF of 1 indicates no correlation.\n",
    "    * A VIF between 5 and 10 suggests moderate correlation.\n",
    "    * A VIF greater than 10 indicates high correlation.\n",
    "\n",
    "**2. Strategies to Address Multicollinearity**\n",
    "\n",
    "* **Remove Highly Correlated Predictors:**\n",
    "    * If two or more variables are highly correlated, remove one of them from the model. \n",
    "    * This simplifies the model and reduces redundancy.\n",
    "    * Use domain knowledge to decide which variable to remove.\n",
    "\n",
    "* **Combine Correlated Predictors:**\n",
    "    * Create a new variable that combines the correlated predictors. \n",
    "    * For example, if \"height\" and \"weight\" are highly correlated, create a new variable \"body mass index\" (BMI).\n",
    "\n",
    "* **Principal Component Analysis (PCA):**\n",
    "    * PCA transforms the original correlated variables into a new set of uncorrelated components.\n",
    "    * These components can then be used as predictors in the regression model.\n",
    "\n",
    "* **Ridge Regression:**\n",
    "    * This technique adds a penalty term to the regression equation that shrinks the coefficients of correlated predictors. \n",
    "    * This helps to stabilize the model and reduce the impact of multicollinearity.\n",
    "\n",
    "* **Lasso Regression:**\n",
    "    * Similar to Ridge Regression, but it can also set the coefficients of some predictors to zero, effectively removing them from the model.\n",
    "\n",
    "* **Increase Sample Size:**\n",
    "    * With a larger sample size, the effects of multicollinearity can be mitigated.\n",
    "\n",
    "**3. Re-evaluate and Iterate**\n",
    "\n",
    "* After implementing any of these strategies, re-evaluate the model:\n",
    "    * Check the VIFs again to see if multicollinearity has been reduced.\n",
    "    * Assess the model's performance using appropriate metrics (e.g., R-squared, adjusted R-squared, RMSE).\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "* **Domain Knowledge:** Understanding the relationships between the variables is crucial in deciding how to address multicollinearity.\n",
    "* **Trade-offs:** Removing or combining variables can lead to information loss. \n",
    "* **Model Interpretation:** If the goal is to understand the individual effects of each predictor, multicollinearity can make interpretation difficult.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e2d29-0119-4b01-8b3d-e8c526818f47",
   "metadata": {},
   "source": [
    "**13) What are some common techniques for transforming categorical variables for use in regression models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec3b02-bd76-470a-9b83-93652c5a9de2",
   "metadata": {},
   "source": [
    "ans. **1. One-Hot Encoding**\n",
    "\n",
    "* **Concept:** Creates a new binary column for each category within the categorical variable. \n",
    "* **Example:** If the categorical variable is \"Color\" with categories \"Red,\" \"Blue,\" and \"Green,\" three new binary columns are created: \"Color_Red,\" \"Color_Blue,\" and \"Color_Green.\" A value of 1 is assigned to the relevant column for each observation, and 0 to the others.\n",
    "* **Advantages:** Simple to understand and implement.\n",
    "* **Disadvantages:** Can increase the number of features significantly, especially with many categories, potentially leading to higher dimensionality and increased computational cost.\n",
    "\n",
    "**2. Label Encoding**\n",
    "\n",
    "* **Concept:** Assigns a unique integer to each category of the categorical variable. \n",
    "* **Example:** If the categorical variable is \"Size\" with categories \"Small,\" \"Medium,\" and \"Large,\" they might be encoded as 0, 1, and 2, respectively.\n",
    "* **Advantages:** Reduces dimensionality compared to one-hot encoding.\n",
    "* **Disadvantages:** May introduce an ordinal relationship between categories that may not exist (e.g., \"Medium\" is not inherently \"greater\" than \"Small\"). This can lead to misleading results in some models.\n",
    "\n",
    "**3. Ordinal Encoding (for ordinal categorical variables)**\n",
    "\n",
    "* **Concept:** Assigns integers to categories based on their order.\n",
    "* **Example:** If the categorical variable is \"Education\" with categories \"High School,\" \"Bachelor's,\" and \"Master's,\" they might be encoded as 1, 2, and 3, respectively.\n",
    "* **Advantages:** Preserves the order of the categories.\n",
    "* **Disadvantages:** Only applicable for ordinal categorical variables.\n",
    "\n",
    "**4. Target Encoding (Mean Encoding)**\n",
    "\n",
    "* **Concept:** Replaces each category with the mean value of the target variable for that category.\n",
    "* **Example:** If the categorical variable is \"City\" and the target variable is \"House Price,\" each city is replaced with the average house price for that city.\n",
    "* **Advantages:** Can capture complex relationships between the categorical variable and the target variable.\n",
    "* **Disadvantages:** Can be prone to overfitting, especially with small datasets. Techniques like smoothing or regularization can help mitigate this.\n",
    "\n",
    "**Choosing the Right Technique:**\n",
    "\n",
    "* **Nature of the Categorical Variable:** Ordinal variables are best suited for ordinal encoding, while nominal variables are typically encoded using one-hot or target encoding.\n",
    "* **Model Assumptions:** Some models (like linear regression) may benefit from specific encoding methods.\n",
    "* **Dataset Size:** One-hot encoding can increase dimensionality, which may be problematic for large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45766e9d-9e6a-4948-aa91-2e4a0bb53efb",
   "metadata": {},
   "source": [
    "**14) What is the role of interaction terms in Multiple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677a1f3-97df-49bf-814f-b24d649863ac",
   "metadata": {},
   "source": [
    "ans. **Interaction Terms in Multiple Linear Regression**\n",
    "\n",
    "**What are Interaction Terms?**\n",
    "\n",
    "In multiple linear regression, interaction terms allow us to model how the effect of one independent variable on the dependent variable changes depending on the value of another independent variable. Essentially, they capture the combined effect of two or more variables that goes beyond their individual effects.\n",
    "\n",
    "**Why are Interaction Terms Important?**\n",
    "\n",
    "1. **Increased Model Flexibility:** Interaction terms allow the regression model to capture more complex relationships between variables, going beyond simple additive effects.\n",
    "2. **Improved Predictive Accuracy:** By incorporating interactions, the model can often provide more accurate predictions of the dependent variable.\n",
    "3. **Deeper Understanding of Relationships:** Interaction terms can reveal important insights into how variables interact with each other to influence the outcome.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a model predicting student performance based on study hours and whether or not they attended a study group.\n",
    "\n",
    "* **Main Effects:**\n",
    "    * Study hours: More study hours are generally associated with higher performance.\n",
    "    * Study group: Students who attend study groups might have higher performance on average.\n",
    "* **Interaction Term:**\n",
    "    * Study hours * Study group: This term captures whether the effect of study hours on performance is different for students who attend study groups compared to those who don't. For example, study groups might amplify the benefits of studying for some students.\n",
    "\n",
    "**Interpreting Interaction Terms:**\n",
    "\n",
    "The coefficient of an interaction term represents the change in the effect of one variable on the dependent variable for a unit change in the other variable. It can be challenging to interpret interaction terms directly, so visualization techniques (e.g., interaction plots) can be helpful.\n",
    "\n",
    "**When to Consider Interaction Terms:**\n",
    "\n",
    "* **Theoretical Basis:** If there's a strong theoretical reason to believe that two variables interact.\n",
    "* **Exploratory Data Analysis:** If scatter plots or other visualizations suggest non-linear or interactive relationships between variables.\n",
    "* **Model Improvement:** If adding interaction terms significantly improves the model's fit (e.g., higher R-squared, lower residual error) and predictive power.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "* **Overfitting:** Including too many interaction terms can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "* **Interpretation:** Interpreting models with multiple interaction terms can become complex.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d9db8-ae44-46a2-9405-99abcdffffc9",
   "metadata": {},
   "source": [
    "**15) How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e5a3a-31c5-4dcd-a779-eb63fb5de4e8",
   "metadata": {},
   "source": [
    "ans. **In Simple Linear Regression:**\n",
    "\n",
    "* **Clear Interpretation:** The intercept represents the predicted value of the dependent variable (Y) when the independent variable (X) is equal to zero. \n",
    "    * **Example:** If you're modeling the relationship between hours studied (X) and exam score (Y), the intercept would represent the predicted exam score when a student studies zero hours.\n",
    "\n",
    "**In Multiple Linear Regression:**\n",
    "\n",
    "* **More Complex Interpretation:** The intercept represents the predicted value of the dependent variable when **all** independent variables are equal to zero. \n",
    "    * **Important Note:** This interpretation might not always be meaningful or realistic. \n",
    "        * **Example:** If you're modeling house prices based on factors like size, age, and number of bedrooms, it's unlikely that a house would have zero size, zero age, and zero bedrooms.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "* **Number of Variables:** In simple regression, the intercept is easier to interpret because it relates to a single independent variable being zero. In multiple regression, it involves all predictors being zero simultaneously.\n",
    "* **Real-World Relevance:** The intercept in multiple regression might not have a direct real-world meaning, as it often represents a hypothetical scenario where all predictors are zero, which might not be possible or meaningful in the context of the data.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "While the concept of the intercept remains the same (the predicted value when all predictors are zero), its interpretation becomes more nuanced and potentially less practical in multiple linear regression due to the involvement of multiple variables.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db3f1a-dd99-4eb3-944a-9123464bd0ef",
   "metadata": {},
   "source": [
    "**16)  What is the significance of the slope in regression analysis, and how does it affect predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177d507-86fb-49ca-9ea5-ef88b9cbe774",
   "metadata": {},
   "source": [
    "ans. The slope in regression analysis is a crucial parameter that quantifies the **rate of change** in the dependent variable (Y) for a unit change in the independent variable (X). Here's its significance and impact on predictions:\n",
    "\n",
    "**Significance of the Slope:**\n",
    "\n",
    "* **Direction of Relationship:**\n",
    "    * **Positive Slope:** Indicates a positive relationship between X and Y. As X increases, Y tends to increase.\n",
    "    * **Negative Slope:** Indicates a negative relationship. As X increases, Y tends to decrease.\n",
    "    * **Zero Slope:** Suggests no linear relationship between X and Y.\n",
    "\n",
    "* **Strength of Relationship:** The magnitude of the slope provides an indication of the strength of the linear relationship. A steeper slope generally implies a stronger relationship.\n",
    "\n",
    "* **Predictive Power:** The slope is fundamental for making predictions using the regression model. It determines how much the predicted value of Y changes for a given change in X.\n",
    "\n",
    "**Impact on Predictions:**\n",
    "\n",
    "* **Accurate Predictions:** A correctly estimated slope is essential for accurate predictions. An incorrect slope will lead to systematic errors in the predictions.\n",
    "* **Sensitivity Analysis:** The slope can be used to assess the sensitivity of the predictions to changes in the independent variable. A larger slope indicates that small changes in X can lead to larger changes in the predicted Y.\n",
    "\n",
    "**In essence, the slope in regression analysis provides valuable insights into the nature and strength of the relationship between variables and plays a critical role in making accurate predictions.**\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say we have a regression model predicting house prices (Y) based on the size of the house (X). If the slope is 100, it means that for every 1 square foot increase in the size of the house, the predicted price is expected to increase by $100, holding other factors constant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe8935-7f69-498f-b166-60eb4d1284ab",
   "metadata": {},
   "source": [
    "**17) How does the intercept in a regression model provide context for the relationship between variables?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0705cf-d50e-4946-a5fb-7f4cfa1a0468",
   "metadata": {},
   "source": [
    "ans. The intercept in a regression model provides valuable context for the relationship between variables by:\n",
    "\n",
    "* **Establishing a Baseline:**\n",
    "    * In simple linear regression, the intercept represents the predicted value of the dependent variable (Y) when the independent variable (X) is zero. \n",
    "    * In multiple linear regression, it represents the predicted value of Y when **all** independent variables are zero.\n",
    "\n",
    "* **Setting the Starting Point:**\n",
    "    * The intercept essentially sets the starting point for the regression line. It determines the position of the line on the y-axis.\n",
    "\n",
    "* **Providing Contextual Interpretation:**\n",
    "    * While not always directly interpretable in real-world terms (especially in multiple regression), the intercept helps understand the overall level of the dependent variable in relation to the independent variables.\n",
    "        * For example, in a model predicting house prices, a high intercept might suggest a generally high housing market in the region, even before considering factors like size or location.\n",
    "\n",
    "* **Guiding Model Interpretation:**\n",
    "    * The intercept, along with the slopes of the independent variables, provides a complete picture of the linear relationship between the variables. \n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "* **Meaningfulness:** In some cases, the intercept might not have a meaningful interpretation in the real world, especially if it's not possible or realistic for all independent variables to be zero.\n",
    "* **Data Limitations:** The intercept is influenced by the range of values in the data. If the data doesn't include values of the independent variables near zero, the intercept might not be reliable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a4265-fc03-4c1c-b1b6-4076e2ef0e56",
   "metadata": {},
   "source": [
    "**18) What are the limitations of using R² as a sole measure of model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1b2a3-7fae-476e-ac46-3482d203758d",
   "metadata": {},
   "source": [
    "ans. R-squared (R²) is a commonly used metric in regression analysis to assess the goodness-of-fit of a model. It represents the proportion of the variance in the dependent variable that is predictable from the independent variables. While R² provides valuable information, it has several limitations:\n",
    "\n",
    "**1. Sensitivity to Outliers:**\n",
    "\n",
    "* Outliers, which are data points that deviate significantly from the general trend, can disproportionately influence the R² value. \n",
    "* A single outlier can artificially inflate or deflate the R² value, leading to misleading conclusions about the model's performance.\n",
    "\n",
    "**2. Overfitting:**\n",
    "\n",
    "* R² tends to increase as more independent variables are added to the model, even if those variables are not meaningful or do not improve the model's predictive power. \n",
    "* This can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "**3. Limited Interpretability:**\n",
    "\n",
    "* A high R² value does not necessarily guarantee that the model is a good representation of the underlying relationship between the variables. \n",
    "* It doesn't provide information about the individual contributions of each independent variable or the significance of the regression coefficients.\n",
    "\n",
    "**4. Assumption of Linearity:**\n",
    "\n",
    "* R² is most appropriate for linear regression models. \n",
    "* If the true relationship between the variables is non-linear, R² may not accurately reflect the strength of the relationship.\n",
    "\n",
    "**5. Doesn't Imply Causation:**\n",
    "\n",
    "* A high R² value indicates a strong correlation between the independent and dependent variables, but it does not imply that the independent variables cause the changes in the dependent variable. \n",
    "* Correlation does not equal causation.\n",
    "\n",
    "**To address these limitations:**\n",
    "\n",
    "* Consider using adjusted R², which penalizes the model for including additional predictors.\n",
    "* Examine residual plots to identify outliers and potential model violations.\n",
    "* Use techniques like cross-validation to assess the model's performance on new data.\n",
    "* Consider other metrics, such as mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE), to evaluate model performance.\n",
    "* Carefully interpret the results and consider the context of the data and research question.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3c714-22c7-474d-9df8-7aba2503d761",
   "metadata": {},
   "source": [
    "**19) How would you interpret a large standard error for a regression coefficient?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb1d20-2b9a-4d93-823f-6370f8957731",
   "metadata": {},
   "source": [
    "ans. A large standard error for a regression coefficient indicates that the estimate of that coefficient is **not very precise**. Here's a breakdown:\n",
    "\n",
    "* **What it Means:**\n",
    "    * **High Uncertainty:** A large standard error implies that there's a high degree of uncertainty surrounding the true value of the coefficient. \n",
    "    * **Noise:** It suggests that the data is noisy, and the relationship between the independent variable and the dependent variable may not be very strong or consistent.\n",
    "    * **Influence of Other Factors:** It could also signify that other factors not included in the model are significantly influencing the relationship.\n",
    "\n",
    "* **Consequences:**\n",
    "    * **Wider Confidence Intervals:** A larger standard error leads to wider confidence intervals for the coefficient. This means we are less certain about the true range of values the coefficient could take.\n",
    "    * **Less Reliable Predictions:** If a coefficient has a large standard error, predictions made using the model will be less reliable, as the uncertainty in the coefficient estimate will propagate into the predictions.\n",
    "    * **Difficulty in Statistical Inference:** A large standard error can make it difficult to determine whether the coefficient is statistically significant. If the standard error is very large, the t-statistic (coefficient divided by standard error) may be small, even if the coefficient itself is not zero.\n",
    "\n",
    "**Possible Causes of Large Standard Errors:**\n",
    "\n",
    "* **Small Sample Size:** With fewer data points, the estimates of the coefficients become more uncertain.\n",
    "* **High Multicollinearity:** If independent variables are highly correlated, it becomes difficult to isolate the unique effect of each variable, leading to larger standard errors.\n",
    "* **Heteroscedasticity:** If the variance of the errors is not constant, it can inflate the standard errors.\n",
    "* **Outliers:** Outliers can significantly impact the regression estimates and increase the standard errors.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "A large standard error for a regression coefficient is a cause for concern. It indicates that our estimate of the true relationship between the variables is uncertain. It's crucial to investigate the potential causes of large standard errors and consider appropriate actions, such as collecting more data, addressing multicollinearity, or transforming the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799d073-39b3-4b9f-8800-eb80006f99ff",
   "metadata": {},
   "source": [
    "**20) How can heteroscedasticity be identified in residual plots, and why is it important to address it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00469fb2-160c-4a94-8e2b-3be856644c3a",
   "metadata": {},
   "source": [
    "ans. **Identifying Heteroscedasticity in Residual Plots**\n",
    "\n",
    "Heteroscedasticity, the unequal variance of residuals across different levels of the independent variable(s), can be visually identified in residual plots. Here's how:\n",
    "\n",
    "* **\"Fan\" or \"Cone\" Shape:**\n",
    "    * **Key Indicator:** If the residual plot exhibits a \"fan\" or \"cone\" shape, it strongly suggests heteroscedasticity. \n",
    "    * **Interpretation:** As the fitted values (predicted values of the dependent variable) increase, the spread of the residuals also increases. This creates a visual pattern resembling a fan or cone.\n",
    "\n",
    "* **Other Patterns:**\n",
    "    * **Increasing/Decreasing Spread:** The spread of residuals might increase or decrease systematically as the fitted values or independent variables change.\n",
    "    * **Clusters of High/Low Residuals:** Observe if there are clusters of residuals with high or low variance in specific regions of the plot.\n",
    "\n",
    "**Why Address Heteroscedasticity?**\n",
    "\n",
    "Addressing heteroscedasticity is crucial because it has significant implications for the reliability and validity of regression model results:\n",
    "\n",
    "1. **Biased Standard Errors:** Heteroscedasticity leads to biased standard errors of the regression coefficients. This means the estimates of how uncertain our coefficient estimates are will be incorrect.\n",
    "\n",
    "2. **Inaccurate Hypothesis Tests:** Biased standard errors directly impact hypothesis tests. We might incorrectly conclude that a coefficient is statistically significant (or insignificant) when it actually isn't.\n",
    "\n",
    "3. **Inefficient Estimates:** The regression coefficients themselves may not be the most efficient estimates possible. This means we might not be getting the best possible fit to the data.\n",
    "\n",
    "**Addressing Heteroscedasticity:**\n",
    "\n",
    "* **Transformations:** Transforming the dependent variable (e.g., taking the logarithm) can sometimes stabilize the variance.\n",
    "* **Weighted Least Squares (WLS):** This technique assigns weights to observations based on their variance, giving more weight to observations with smaller variances.\n",
    "* **Robust Standard Errors:** These standard errors are less sensitive to heteroscedasticity and can provide more reliable inference.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "By carefully examining residual plots for patterns like the \"fan\" or \"cone\" shape, you can effectively identify heteroscedasticity. Addressing this issue is essential for obtaining reliable and accurate results from your regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c46af-9cab-46ef-bac1-27af740e4184",
   "metadata": {},
   "source": [
    "**21) What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4079f7d-bf3b-45ab-aa30-2e4c59d8ce2c",
   "metadata": {},
   "source": [
    "ans. In a multiple linear regression model, a high R-squared value but a low adjusted R-squared value suggests that **the model may be overfitting the data**. \n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "* **R-squared:** This metric measures the proportion of variance in the dependent variable that's explained by the independent variables in the model. \n",
    "    * A high R-squared indicates that the model explains a large portion of the variability in the data.\n",
    "\n",
    "* **Adjusted R-squared:** This metric is a modified version of R-squared that accounts for the number of predictors in the model. \n",
    "    * It penalizes the inclusion of unnecessary predictors.\n",
    "    * It only increases if the newly added predictor improves the model's predictive power.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* If R-squared is high but adjusted R-squared is low, it suggests that:\n",
    "    * The model may be including too many independent variables, some of which may not be significantly related to the dependent variable.\n",
    "    * These extra variables are increasing the model's complexity and potentially leading to overfitting.\n",
    "    * Overfitting occurs when the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "* The high R-squared indicates that the model explains a significant portion of the variance in the training data.\n",
    "* However, the low adjusted R-squared suggests that this high R-squared value may be inflated due to the inclusion of irrelevant predictors.\n",
    "\n",
    "**What to do:**\n",
    "\n",
    "* **Model Simplification:** Consider removing some of the less important predictors from the model.\n",
    "* **Feature Selection:** Use techniques like stepwise regression or cross-validation to identify the most important predictors.\n",
    "* **Regularization:** Techniques like Ridge regression or Lasso regression can help to prevent overfitting by shrinking the coefficients of less important predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7a6e8-bbaf-4e49-a9f5-af9138d3bd11",
   "metadata": {},
   "source": [
    "**22)  Why is it important to scale variables in Multiple Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12803c78-bdc9-4cb9-a6e3-fec310f43f11",
   "metadata": {},
   "source": [
    "ans. **Scaling variables in multiple linear regression is crucial for several reasons:**\n",
    "\n",
    "1. **Improved Gradient Descent Convergence:**\n",
    "\n",
    "   - **Unequal Scales:** When independent variables have vastly different scales (e.g., one variable ranges from 0 to 1, another from 100 to 10000), the gradient descent optimization algorithm can take longer to converge. \n",
    "   - **Reason:** Gradient descent updates the model's parameters based on the gradients of the loss function. Unequal scales can lead to:\n",
    "      - **Slow Updates:** The algorithm might take very small steps along the dimensions of variables with larger scales, slowing down the overall convergence process.\n",
    "      - **Oscillations:** The algorithm might oscillate back and forth, failing to find the optimal solution efficiently.\n",
    "\n",
    "2. **Better Regularization Performance:**\n",
    "\n",
    "   - **Regularization Techniques:** Techniques like Ridge and Lasso regression add a penalty term to the loss function to prevent overfitting. \n",
    "   - **Scale Sensitivity:** These penalties are sensitive to the scale of the features. If features have different scales, the regularization term might penalize some features more heavily than others, even if they have similar importance.\n",
    "   - **Improved Results:** Scaling ensures that all features contribute fairly to the regularization process, leading to better model performance and potentially improved generalization.\n",
    "\n",
    "3. **Enhanced Interpretability (Sometimes):**\n",
    "\n",
    "   - **Standardization:** When variables are standardized (mean 0, standard deviation 1), the coefficients can be more easily compared. \n",
    "   - **Interpretation:** In a standardized model, the coefficient of a predictor represents the change in the dependent variable associated with a one-standard-deviation change in the predictor.\n",
    "\n",
    "**Key Scaling Methods:**\n",
    "\n",
    "* **Standardization (Z-score normalization):** Transforms the data to have zero mean and unit variance.\n",
    "* **Normalization (Min-Max scaling):** Scales the data to a specific range, typically between 0 and 1.\n",
    "\n",
    "**When to Scale:**\n",
    "\n",
    "* **Gradient Descent-based Optimization:** Highly recommended.\n",
    "* **Regularization Techniques:** Essential for optimal performance.\n",
    "* **Comparing Feature Importance:** Can be helpful for interpreting the relative importance of different features.\n",
    "\n",
    "**When Not to Scale (Generally):**\n",
    "\n",
    "* **Tree-based Models:** Tree-based models (like decision trees and random forests) are generally not sensitive to feature scaling.\n",
    "* **When Interpretability in Original Units is Critical:** If you need to interpret the coefficients in the original units of the features, scaling might not be desirable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65709b-09d8-42b0-bb9a-f6a8a62433ec",
   "metadata": {},
   "source": [
    "**23) What is polynomial regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9202a3-6c57-4a21-a2b9-ebb49f3e1200",
   "metadata": {},
   "source": [
    "ans. **Polynomial Regression**\n",
    "\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth degree polynomial in x. \n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Non-linear Relationships:** Unlike simple linear regression, which assumes a straight-line relationship, polynomial regression can capture curved patterns in the data.\n",
    "* **Polynomial Equation:** The model is represented by a polynomial equation, such as: \n",
    "   * **Quadratic:** y = b0 + b1*x + b2*x^2\n",
    "   * **Cubic:** y = b0 + b1*x + b2*x^2 + b3*x^3 \n",
    "   * **Higher-order polynomials** with higher powers of x\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "* **Flexibility:** By including higher-order terms (x^2, x^3, etc.), the model can fit more complex curves to the data.\n",
    "* **Curve Fitting:** The model aims to find the coefficients (b0, b1, b2, ...) that best fit the data points by minimizing the difference between the predicted values and the actual values.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Modeling Complex Relationships:** In situations where the relationship between variables is not linear, polynomial regression can provide a more accurate representation.\n",
    "* **Examples:**\n",
    "    * **Predicting stock prices:** Stock prices often exhibit non-linear trends.\n",
    "    * **Modeling the growth of organisms:** Growth rates can follow non-linear patterns.\n",
    "    * **Analyzing the effect of temperature on chemical reactions:** Chemical reactions may have non-linear responses to temperature changes.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **Flexibility:** Can capture complex non-linear relationships.\n",
    "* **Improved Accuracy:** Often provides better predictions than linear regression when the relationship is non-linear.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* **Overfitting:** Higher-order polynomials can overfit the data, meaning they perform well on the training data but poorly on new, unseen data.\n",
    "* **Interpretation:** Interpreting the coefficients of higher-order terms can be challenging.\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "Polynomial regression is a valuable tool for modeling non-linear relationships between variables. By incorporating higher-order terms, it provides greater flexibility than simple linear regression and can improve the accuracy of predictions. However, it's crucial to carefully choose the degree of the polynomial to avoid overfitting and to interpret the results with caution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d46f8-d224-4ad7-8d19-57c56e4a36dc",
   "metadata": {},
   "source": [
    "**24) How does polynomial regression differ from linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dc310-6afd-4bcb-9651-3d89c2741f75",
   "metadata": {},
   "source": [
    "ans. **Key Differences Between Linear and Polynomial Regression**\n",
    "\n",
    "| Feature | Linear Regression | Polynomial Regression |\n",
    "|---|---|---|\n",
    "| **Relationship Type** | Assumes a linear relationship between the independent and dependent variables. | Assumes a non-linear relationship, often curved. |\n",
    "| **Equation** | Represented by a straight line: Y = a + bX | Represented by a polynomial equation (e.g., quadratic: Y = a + bX + cX^2, cubic: Y = a + bX + cX^2 + dX^3) |\n",
    "| **Flexibility** | Less flexible, can only model linear relationships. | More flexible, can capture complex curves and patterns in the data. |\n",
    "| **Overfitting** | Less prone to overfitting. | More prone to overfitting, especially with higher-order polynomials. |\n",
    "| **Interpretation** | Generally easier to interpret the coefficients (slope and intercept). | Can be more challenging to interpret the coefficients of higher-order terms. |\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "* **Linear Regression:** Fits a straight line to the data.\n",
    "* **Polynomial Regression:** Fits a curve (e.g., parabola, cubic) to the data.\n",
    "\n",
    "**When to Use Which:**\n",
    "\n",
    "* **Linear Regression:** Suitable when the relationship between variables is expected to be linear.\n",
    "* **Polynomial Regression:** Suitable when the relationship is non-linear, such as when the data exhibits curves or patterns that cannot be adequately captured by a straight line.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* **Linear:** Predicting house prices based solely on the size of the house, assuming a linear relationship between size and price.\n",
    "* **Polynomial:** Predicting stock prices over time, as stock prices often exhibit non-linear trends with fluctuations and curves.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "Polynomial regression offers greater flexibility than linear regression by allowing for non-linear relationships. However, it's important to be mindful of overfitting and choose the appropriate degree of the polynomial to avoid overly complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e2d50-6f5c-4b34-8b98-14d86ba79bde",
   "metadata": {},
   "source": [
    "**25) When is polynomial regression used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6ff86-8104-4bf2-ae0f-ea18daea0030",
   "metadata": {},
   "source": [
    "ans. Polynomial regression is used when the relationship between the independent variable (x) and the dependent variable (y) is **not linear**. \n",
    "\n",
    "Here are some key situations where polynomial regression is particularly valuable:\n",
    "\n",
    "* **Modeling Complex Curves:** When the data exhibits a curved pattern, such as:\n",
    "    * **U-shaped or inverted U-shaped relationships:** For example, the relationship between fertilizer application and crop yield might initially increase with fertilizer but then decrease at very high levels of application.\n",
    "    * **S-shaped curves:** Many growth processes, like population growth or the spread of diseases, follow S-shaped curves.\n",
    "\n",
    "* **Capturing Non-Linear Trends:** In fields like finance, stock prices often exhibit non-linear trends with fluctuations and cycles. Polynomial regression can help capture these complex patterns.\n",
    "\n",
    "* **Improving Predictive Accuracy:** When a linear regression model fails to adequately capture the relationship between variables, polynomial regression can often provide more accurate predictions.\n",
    "\n",
    "* **Analyzing Scientific Data:** In various scientific fields, such as physics, chemistry, and biology, many phenomena exhibit non-linear relationships. Polynomial regression can be used to model these relationships and gain insights into the underlying processes.\n",
    "\n",
    "* In essence, polynomial regression is a powerful tool when the data suggests a non-linear relationship between the variables, allowing for a more accurate and nuanced understanding of the underlying patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bd90f-bcdc-4963-96f3-d4b3f5b587c3",
   "metadata": {},
   "source": [
    "**26) What is the general equation for polynomial regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac74c1-83c4-4491-aae9-ea8be4c0b164",
   "metadata": {},
   "source": [
    "ans. The general equation for polynomial regression of degree 'n' is:\n",
    "\n",
    "**y = β₀ + β₁x + β₂x² + β₃x³ + ... + βₙxⁿ + ε**\n",
    "\n",
    "Where:\n",
    "\n",
    "* **y:** The dependent variable \n",
    "* **x:** The independent variable\n",
    "* **β₀, β₁, β₂, ..., βₙ:** The coefficients of the polynomial terms\n",
    "* **x², x³, ..., xⁿ:** Higher-order terms of the independent variable\n",
    "* **ε:** The error term\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Degree of the Polynomial:** The highest power of 'x' in the equation determines the degree of the polynomial. \n",
    "    * For example:\n",
    "        * **Linear Regression:** Degree 1 (y = β₀ + β₁x + ε)\n",
    "        * **Quadratic Regression:** Degree 2 (y = β₀ + β₁x + β₂x² + ε)\n",
    "        * **Cubic Regression:** Degree 3 (y = β₀ + β₁x + β₂x² + β₃x³ + ε) \n",
    "\n",
    "* **Flexibility:** By increasing the degree of the polynomial, the model becomes more flexible and can capture more complex non-linear relationships between the variables. However, increasing the degree also increases the risk of overfitting.\n",
    "\n",
    "This general equation allows you to model various curved relationships between the independent and dependent variables by adjusting the degree of the polynomial and the corresponding coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2a2ec-6d0d-4681-b8c6-3f838d6512fa",
   "metadata": {},
   "source": [
    "**27) Can polynomial regression be applied to multiple variables?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291093ba-8cf7-472c-954d-284009970908",
   "metadata": {},
   "source": [
    "ans. Yes, polynomial regression can be extended to handle multiple independent variables. This is often referred to as **multivariate polynomial regression**.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "* **Multiple Independent Variables:** Instead of just one independent variable (x), we consider multiple independent variables (x1, x2, x3, ...).\n",
    "* **Polynomial Terms:** The model includes not only the individual variables but also their higher-order terms (squares, cubes, etc.) and interaction terms between the variables.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a model with two independent variables, x1 and x2. A second-degree polynomial regression model for this case could look like:\n",
    "\n",
    "**y = β₀ + β₁x₁ + β₂x₂ + β₃x₁² + β₄x₂² + β₅x₁x₂ + ε**\n",
    "\n",
    "* **Interaction Term:** The term β₅x₁x₂ represents the interaction between x1 and x2. It captures how the effect of x1 on y changes depending on the value of x2, and vice versa.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "* **Increased Complexity:** As the number of independent variables and the degree of the polynomial increase, the model becomes more complex and the number of terms in the equation grows rapidly.\n",
    "* **Overfitting:** The risk of overfitting increases significantly with multivariate polynomial regression due to the large number of potential terms. Careful model selection and techniques like regularization (e.g., Ridge, Lasso) are crucial.\n",
    "* **Interpretation:** Interpreting the coefficients of higher-order terms and interaction terms can be challenging.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "Multivariate polynomial regression extends the concept of polynomial regression to handle multiple independent variables. It allows for modeling complex, non-linear relationships between multiple predictors and the dependent variable. However, it's crucial to carefully consider the potential for overfitting and the interpretability of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c5768-1b07-42d3-9b96-cebebec915b9",
   "metadata": {},
   "source": [
    "**28) What are the limitations of polynomial regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c9b6b-2a6f-4da6-842d-d80cb68b803b",
   "metadata": {},
   "source": [
    "ans. **Key Limitations of Polynomial Regression:**\n",
    "\n",
    "* **Overfitting:** \n",
    "    * This is the most significant limitation. As the degree of the polynomial increases, the model becomes more flexible and can fit the training data extremely well, capturing even noise and random fluctuations. \n",
    "    * This leads to poor generalization, meaning the model performs poorly on new, unseen data.\n",
    "    * Overfitting can be mitigated by techniques like:\n",
    "        * **Cross-validation:** To assess model performance on unseen data.\n",
    "        * **Regularization:** Techniques like Ridge and Lasso regression can help prevent overfitting by penalizing complex models.\n",
    "        * **Careful selection of the polynomial degree:** Choosing an appropriate degree is crucial. Techniques like cross-validation can help determine the optimal degree.\n",
    "\n",
    "* **Interpretability:**\n",
    "    * Higher-order polynomials can be difficult to interpret. \n",
    "    * The coefficients of higher-order terms may not have straightforward interpretations in terms of the relationship between the variables.\n",
    "    * This can make it challenging to understand the underlying mechanisms and draw meaningful conclusions from the model.\n",
    "\n",
    "* **Computational Complexity:**\n",
    "    * Higher-degree polynomials can increase the computational complexity of the model, especially with large datasets. \n",
    "    * This can lead to longer training times and increased resource requirements.\n",
    "\n",
    "* **Data Requirements:**\n",
    "    * Polynomial regression generally requires a sufficient amount of data to accurately estimate the coefficients of higher-order terms. \n",
    "    * With limited data, the model may be prone to overfitting.\n",
    "\n",
    "* **Extrapolation:**\n",
    "    * Polynomial regression models may not accurately predict values outside the range of the observed data, especially with high-degree polynomials. \n",
    "    * Extrapolation can lead to unreliable and potentially erroneous predictions.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "While polynomial regression is a powerful tool for capturing non-linear relationships, it's crucial to be aware of these limitations and take appropriate steps to mitigate them. Careful model selection, validation, and interpretation are essential for obtaining reliable and meaningful results from polynomial regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd08de5-7065-4a81-9277-f36f6cb7720c",
   "metadata": {},
   "source": [
    "**29) What methods can be used to evaluate model fit when selecting the degree of a polynomial ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81f58d-74e2-40b1-879d-b1897cce1f59",
   "metadata": {},
   "source": [
    "ans. **Here are some key methods to evaluate model fit when selecting the degree of a polynomial:**\n",
    "\n",
    "**1. Cross-Validation:**\n",
    "\n",
    "* **Principle:** Divide the data into multiple subsets (folds). Train the model on a portion of the data (training set) and evaluate its performance on the remaining data (validation set). Repeat this process multiple times, using different subsets for validation each time.\n",
    "* **Metrics:** Calculate metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), or R-squared on the validation sets.\n",
    "* **Selection:** Choose the polynomial degree that results in the best average performance across all folds.\n",
    "\n",
    "**2. Train-Test Split:**\n",
    "\n",
    "* **Principle:** Divide the data into a training set and a test set. Train the model on the training set and evaluate its performance on the unseen test set.\n",
    "* **Metrics:** Use the same metrics as in cross-validation (MSE, RMSE, MAE, R-squared) to assess performance on the test set.\n",
    "* **Selection:** Choose the degree that results in the best performance on the test set.\n",
    "\n",
    "**3. Adjusted R-squared:**\n",
    "\n",
    "* **Principle:** Adjusted R-squared penalizes the model for including additional predictors (polynomial terms).\n",
    "* **Selection:** Choose the degree that results in the highest adjusted R-squared value.\n",
    "\n",
    "**4. Visual Inspection:**\n",
    "\n",
    "* **Principle:** Plot the fitted polynomial curve along with the data points. \n",
    "* **Selection:** Visually inspect the fit. Look for:\n",
    "    * **Overfitting:** The curve may exhibit excessive wiggles and closely follow noise in the data.\n",
    "    * **Underfitting:** The curve may not capture the underlying trend in the data.\n",
    "    * **Good Fit:** The curve should capture the general trend in the data without overfitting.\n",
    "\n",
    "**5. Information Criteria:**\n",
    "\n",
    "* **Principle:** Use criteria like Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). These criteria balance model fit with model complexity.\n",
    "* **Selection:** Choose the degree that minimizes the AIC or BIC value.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "* **Bias-Variance Trade-off:** Higher-degree polynomials can reduce bias (better fit the training data) but increase variance (overfitting).\n",
    "* **Domain Knowledge:** Consider the underlying process and whether a high-degree polynomial is reasonable based on your understanding of the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8af4a-0c3b-4d31-9759-0c71c8d69913",
   "metadata": {},
   "source": [
    "**30) Why is visualization important in polynomial regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8e924-f7fd-4d0c-bd8f-c0daed4a9259",
   "metadata": {},
   "source": [
    "ans. Visualization is crucial in polynomial regression for several reasons:\n",
    "\n",
    "* **Identifying Non-Linear Relationships:**\n",
    "    * **Initial Exploration:** Scatter plots of the dependent variable against the independent variable quickly reveal whether a linear relationship is sufficient or if a non-linear model is necessary. \n",
    "    * **Curvature Detection:** Visual inspection helps identify if the data exhibits curves, bends, or other non-linear patterns that suggest the need for polynomial regression.\n",
    "\n",
    "* **Selecting the Degree of the Polynomial:**\n",
    "    * **Overfitting/Underfitting:** Plotting the fitted polynomial curve alongside the data allows you to visually assess:\n",
    "        * **Overfitting:** If the curve exhibits excessive wiggles, closely following noise in the data, it suggests overfitting.\n",
    "        * **Underfitting:** If the curve fails to capture the underlying trend in the data, it suggests underfitting.\n",
    "    * **Optimal Degree:** By visually comparing the fits of different polynomial degrees, you can select the degree that best captures the underlying trend without overfitting.\n",
    "\n",
    "* **Assessing Model Fit:**\n",
    "    * **Residual Plots:** Visualizing the residuals (the differences between the actual and predicted values) can help identify potential issues:\n",
    "        * **Heteroscedasticity:** If the spread of residuals increases or decreases systematically, it suggests heteroscedasticity, a violation of the model assumptions.\n",
    "        * **Outliers:** Outliers can significantly impact the fit of the polynomial curve and can be identified visually.\n",
    "\n",
    "* **Communicating Results:**\n",
    "    * **Clearer Explanations:** Visualizations, such as scatter plots with the fitted polynomial curves, make it easier to communicate the model's findings to others. \n",
    "    * **Better Understanding:** Visualizations provide a more intuitive understanding of the relationship between the variables and how the polynomial model captures that relationship.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "Visualization plays a vital role in all stages of polynomial regression, from initial data exploration and model selection to assessing model fit and communicating results. By effectively utilizing visualizations, you can gain valuable insights into the data, make informed decisions about model complexity, and ensure the reliability and interpretability of your polynomial regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634569a0-ec47-4298-a785-9046d9e6efcf",
   "metadata": {},
   "source": [
    "**31) How is polynomial regression implemented in Python?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fdd4b5-bfb7-4750-9221-3e2bd86378e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.380606060606059\n",
      "R² Score: 0.9936681064914417\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([3, 5, 7, 10, 15, 19, 27, 30, 38, 50])\n",
    "\n",
    "# Degree of polynomial\n",
    "degree = 2\n",
    "\n",
    "# Polynomial transformation\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X, y, color='blue', label='Original Data')\n",
    "plt.plot(X, y_pred, color='red', label=f'Polynomial Regression (degree={degree})')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec9f5f-b4ab-4606-bd2d-4eebf4602829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
